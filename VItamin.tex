%% Template for a preprint Letter or Article for submission
%% to the journal Nature.
%%

\documentclass[%
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
showpacs,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
 twocolumn,
 prl,
 reprint,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{lineno}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{acronym}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{hyperref}
%\usepackage{booktabs}

\hypersetup{
%--- fill inside borders ---
  colorlinks=true,        % false: boxed links; true: colored links
  linkcolor=black,         % color of internal links
  citecolor=cyan,         % color of links to bibliography
}

%% ----- comment commands for each of us
\newcommand{\chris}[1]{\textbf{\textcolor{red}{CHRIS: #1}}}
\newcommand{\francesco}[1]{\textbf{\textcolor{green}{FRANCESCO: #1}}}
\newcommand{\hunter}[1]{\textbf{\textcolor{blue}{HUNTER: #1}}}
\newcommand{\siong}[1]{\textbf{\textcolor{cyan}{SIONG: #1}}}
\newcommand{\rod}[1]{\textbf{\textcolor{yellow}{ROD: #1}}}

\begin{document}

\preprint{APS/123-QED}

\title{Estimating Bayesian parameter estimation using conditional variational
autoencoders for gravitational-wave astronomy}

\author{Hunter Gabbard$^1$}
 \email{Corresponding author: h.gabbard.1@research.gla.ac.uk}
\author{Chris Messenger$^1$}
\author{Ik Siong Heng$^1$}
\author{Francesco Tonolini$^2$}
\author{\& Roderick Murray-Smith$^2$}

\affiliation{
 SUPA, School of Physics and Astronomy$^1$, \\
 University of Glasgow, \\
 Glasgow G12 8QQ, United Kingdom \\ \\
 School of Computing Science$^2$, \\
 University of Glasgow, \\
 Glasgow G12 8QQ, United Kingdom \\
}

\date{\today}

\maketitle

\acrodef{GW}[GW]{gravitational wave}
\acrodef{BBH}[BBH]{binary black hole}
\acrodef{EM}[EM]{electromagnetic}
\acrodef{CBC}[CBC]{compact binary coalescence}
\acrodef{BNS}[BNS]{binary neutron star}
\acrodef{NSBH}[NSBH]{neutron star black hole}
\acrodef{SNR}[SNR]{signal-to-noise ratio}
\acrodef{PSD}[PSD]{power spectral density}
\acrodef{FFT}[FFT]{fast Fourier transform}
\acrodef{CNN}[CNN]{convolutional neural network}
\acrodef{ROC}[ROC]{receiver operator characteristic}
\acrodef{ELBO}[ELBO]{evidence lower bound}
\acrodef{LIGO}[LIGO]{advanced Laser Interferometer Gravitational wave Observatory}
\acrodef{CVAE}[CVAE]{conditional variational autoencoder}
\acrodef{AD}[AD]{Anderson-Darling}
\acrodef{KS}[KS]{Kolmogorov-Smirnoff}
\acrodef{KL}[KL]{Kullbackâ€“Leibler}
\acrodef{GPU}[GPU]{graphics processing unit}
\acrodef{LVC}[LVC]{LIGO-Virgo Collaboration}

%
% Introductory paragraph describing the content of the letter
%
% This format begins with a title of, at most, 15 words, followed by an
% introductory paragraph (not abstract) of approximately 150 words, summarizing
% the background, rationale, main results (introduced by "Here we show" or some
% equivalent phrase) and implications of the study. This paragraph should be
% referenced, as in Nature style, and should be considered part of the main
% text, so that any subsequent introductory material avoids too much redundancy
% with the introductory paragraph.
%
\textbf{ 
%
% background
%
The third \ac{LIGO} and Virgo observing run is well under way and we are now in
an era where \ac{GW} detection is commonplace~\cite{PhysRevLett.116.061102,
PhysRevX.6.041015,PhysRevLett.119.161101}. As the sensitivity of the global
network of \ac{GW} detectors improves, we will observe $\mathcal{O}(100)$s of
transient \ac{GW} events per year~\cite{1409.7215}. For each of these events
the current methods used to estimate their source parameters employ optimally
sensitive~\cite{2009CQGra..26o5017S} but computationally costly Bayesian inference
approaches~\cite{1409.7215}.
%
% rationale
%
For \ac{BBH} signals, existing \ac{GW} analyses can take $\mathcal{O}(10^{5} -
10^{6})$ seconds to complete~\cite{1409.7215} and for \ac{BNS}
signals this timescale increases by at least an order of magnitude
~\cite{PhysRevLett.119.161101}. It is for this latter class of signal (and
\ac{NSBH} systems) that counterpart \ac{EM} signatures are expected, containing
prompt emission on timescales of 1 second -- 1 minute. The current fastest
method for alerting \ac{EM} follow-up observers~\cite{2016PhRvD..93b4013S}, can
provide estimates in $\mathcal{O}(1)$ minute, on a limited range of key source
parameters. Given the deluge of expected signals, and the scientifc gains to be
made through speed improvement it is imperative that a significantly more
efficient parameter estimation method be developed.
%
% results
%
Here we show that a \ac{CVAE}~\cite{1904.06264,1812.04405} pre-trained on
\ac{BBH} signals and without being given the precomputed posteriors can return
Bayesian posterior probability estimates on source parameters. The training
procedure need only be performed once for a given prior parameter space (or
signal class) and the resulting trained machine can then generate samples
describing the posterior distribution $\sim 7$ orders of magnitude faster than
existing techniques.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% introduction - this section has to expand upon what has mentioned in the
% abstract background (which was only ~50 words). It needs to cover the state of
% the gravitational wave field and the number of detections expected in the next
% ~5 years. It should briefly discuss the issue of low latency EM follow up. It
% needs to cover Bayesian inference (not in too much detail) and the signal model
% we are interested in here (again, not too much detail but enough for the
% average Nature reader). It then needs to introduce machine learning and focus
% mainly on how our scheme works. We also need to include a statement about how
% the training data priors affect the result (are they really the priors?)
%
% Intro to the detection era with the LVC
%
With the overwhelmingly successful observation runs of O1 and O2 now complete,
\ac{LIGO} and Virgo have produced a large catalogue of \ac{GW} data covering
both \ac{BBH} and \ac{BNS} signals~\cite{1811.12907}. Over the next five years
we expect the number of detections to increase to be upwards of $\sim180$
\ac{BNS} and $\sim400$ BBH events per year~\cite{1304.0670,1811.12907}. This
large influx in the number of detections will put an increased amount of
pressure on the current \ac{GW} inference methods used for parameter
estimation.  

%
% From GW detection, to parameter estimation
%
Much of the \ac{LIGO} analysis effort over the past several years has been
focused on the detection of \acp{GW}. This has been the primary focus of many,
due to the difficulty associated with identifying \ac{GW} waveforms buried in
in large amount of noise to a high degree of certainty. The detection problem
has largely been solved through the use of matched template
filtering~\cite{0264-9381-33-21-215004}. Once a \ac{GW} has been identified
through matched template filtering, Bayesian inference is used to extract
information about the source parameters of the detected \ac{GW} event.

%
% Set up parameter estimation problem
%
In the standard Bayesian \ac{GW} inference approach, we assume that we are
given both a signal model and a noise model. Both the signal and the 
noise model may have unknown parameters we are interested in inferring. 
Each parameter is given a prior astrophysically motivated probability 
distribution. In our case, we have additive noise which is modelled as 
a Gaussian (in reality, the data is not truly Gaussian). Given a noisy
\ac{GW} waveform, we would like to find an optimal procedure for retrieving
some finite set of unknown \ac{GW} parameters. Our procedure should be able
to give us an accurate estimate of the parameters of our observed signal, while
also accounting for the uncertainty which arises from having multiple noise
realizations of our observed data able to be mapped to one parameter
estimate.

%
% Describe Bayes Theorem
%
According to Bayes Theorem, a posterior for a set of parameters can be
represented as
%
\begin{align}\label{eq:bayes_theorem}
    p(x|y) &\propto p(y|x) p(x),
\end{align}
%
where $x$ are the parameters, $y$ is the observed data, $p(x|y)$ is the
posterior, $p(y|x)$ is the likelihood, and $p(x)$ is the prior we put on our
parameter distribution. The constant of proportionality, which we omit here, is
$p(y)$, the probability of our data, known as the Bayesian evidence or the
marginal likelihood. We typically ignore $p(y)$ since it is a constant and for
parameter estimation purposes we are only interested in the shape of the
posterior.

Due to the size of the parameter space typically encountered in \ac{GW}
parameter estimation and the large quantities of data used, we must
stochastically sample the parameter space in order to estimate the posterior.
Sampling is done using a variety of techniques including nested
sampling~\cite{cpnest,dynesty} and Markov chain Monte Carlo
methods~\cite{emcee,ptemcee}. The primary software tools used by the \ac{LIGO}
parameter estimation analysis are \texttt{LALInference} and
\texttt{Bilby}~\cite{1409.7215,1811.02042}, which contain many different
sampling methods.  
  
%
% Intro to machine learning section
%
Machine learning has featured prominently in many areas of gravitational wave
research over the last few years. These techniques have shown to be
particularly promising in signal detection
\cite{GEORGE201864,PhysRevLett.120.141103,1904.08693}, glitch classification
\cite{1706.07446,0264-9381-34-6-064003} and earthquake prediction
\cite{Coughlin_2017}. Recently, a type of neural network known as \ac{CVAE} was
shown to perform exceptionally well when applied towards computational imaging
inference~\cite{1904.06264,NIPS2015_5775}, text to image
inference~\cite{1512.00570}, high-resolution synthetic image
generation~\cite{1612.00005} and the fitting of incomplete heterogeneous
data~\cite{1807.03653}. It is this type of machine learning network that we
utilize to accuratelly approximate the Bayesian posterior $p(x|y)$, which is
known to be the optimal result ~\cite{2009CQGra..26o5017S}.

%
% What is an autoencoder?
%
Conditional variational autoencoders are a form of variational autoencoder
which are conditioned on an observation, where in our case the observation is a
1-dimensional \ac{GW} time series signal $y$. The autoencoders from which
variational autoencoders are derived are typically used for problems involving
image reconstruction and/or dimensionality reduction. They perform a regression
task whereby the autoencoder attempts to predict its own given input (model the
identity function) through a limited and therefore distilled representation of
the input parameter space. An autoencoder is composed of two neural networks,
an encoder and a decoder~\cite{LIOU20083150}. The encoder network takes as
input an vector, where the number of dimensions is a fixed number predefined by
the user. The encoder converts the input vector into a (typically) lower
dimensional space, referred to as the {\it{latent space}}. A representation of
the data in the latent space is passed to the decoder network which generates a
reconstruction of the original input data to the encoder network. Through
training, the two sub-networks learn a how to efficiently represent a dataset
within a lower dimensional latent space which will take on the most important
properties of our input training data. In this way, the data can be compressed
with little loss of fidelity. Additionally, the decoder simultaneously learns
to decode the latent space representation and reconstruct that data back to its
original form (the input data). 

%
% What is a variational autoencoder?
%
The primary difference between a variational autoencoder~\cite{1812.04405} and
an autoencoder concerns the method by which locations within the latent space
are produced. In our variant of the variational autoender, the output of the
encoder is interpreted as a set of parameters governing statistical
distributions (in our case the means and variances of multivariant Gaussians).
In proceeding to the decoder network, samples are randomly drawn from these
distributions ($z$) and fed into the decoder, therefore adding an element of
variation into the process. Therefore a particular input can have infinitely
many outputs. In both the decoder and the encoder networks we use
fully-connected layers (although this is not a constraint and any trainable
network architecture may be used).

%
% Brief introduction to loss functions used in the neural networks
%
The construction of a \ac{CVAE} begins with the definition of a quantity to be
minimised (referred to as a loss or cost function). We can relate that aim to
that of approximating the posterior distribution by minimising the cross
entropy, defined as
%
\begin{align}\label{eq:cross_ent} 
H(p,r) &= -\int dx\, p(x|y) \log r_{\theta}(x|y) 
\end{align}
%
between the true posterior $p(x|y)$ and $r_{\theta}(x|y)$, the parametric
distribution that we will use neural networks to construct and which we aim to
be equal to the true posterior. In this case $\theta$ represents a set of
trainable neural network parameters. Starting from this point it is possible to
derive a computable form for the cross-entropy that is reliant on a set of
unknown functions that can be modelled by the components of variational encoder
and decoder neural networks. The details of the derivation are described in
Sec.~\ref{sec:methods} and in~\cite{1904.06264}. The final form of the
cross-entropy loss function is given by the bound
%
\begin{align}\label{eq:cost3}
H \lesssim -\frac{1}{N}\sum_{n=1}^{N}&\left[\log
r_{\theta_{2}}(x_{n}|z_{n},y_{n})\right.\nonumber\\
&\left.-\text{KL}\left[q_{\phi}(z|x_{n},y_{n})||r_{\theta_{1}}(z|y_{n})\right]\right].
\end{align}
%
and requires three fully-connected networks; two encoder networks (labelled
$\textrm{E}_1$, $\textrm{E}_2$ in Fig.~\ref{fig:network_config}) representing
the functions $r_{\theta_{1}}(z|y)$ and $q_{\phi}(z|x,y)$ respectively, and one
decoder network (D) representing the function $r_{\theta_{2}}(x|z,y)$. The
function $\text{KL}(\cdot||\cdot)$ denotes the Kulback-Leibler divergence. In
practice, during the training procedure the various integrations that are part
of the derivation are approximated by a sum over $N$ training data samples at
each stage of training. Training is performed via a series of steps detailed in
Sec.~\ref{sec:methods}.

\begin{figure}
    \includegraphics[width=\columnwidth]{images/network_setup.png}
    \caption{\label{fig:network_config} This figure illustrates our neural
network training/testing operations. During training (left-hand side), 
a training set of noisy
\ac{GW} signals ($y$) and their corresponding true parameters ($x$) are given
as input to encoder network $\textrm{E}_2$, while only $y$ is 
given to $\textrm{E}_1$. The K-L divergence (Eq.~\ref{eq:kl}) is computed
between the moments of latent space representations. We then sample from a
multivariate Gaussian whose mean and variances are defined by $\mu^{j}_{2}$, 
which is given as input to the decoder network. We take the first two
moments $\mu^{j}_x$ of the predictions from the decoder network, along with our
training parameters $x$ and compute a cost function Eq. \ref{eq:cost}. After
having trained our networks, we test (right-hand side) using only $\textrm{E}_1$ and the decoder.
The decoder returns a multi-dimensional Gaussian function that describes 
the likelihood/posterior and so we simply sample from it. A more 
detailed description of the network can be found in the letter text.} 
\end{figure}

%
% Some practical aspects of the training
%
The losses computed from the K-L divergence and the cost function are summed
and then back-propagated through all three networks (E1, E2, and D).  As is
standard practice in machine learning applications, the loss is computed over a
batch of training samples and repeated for a pre-defined number of iterations.
For our purposes, we found that $\sim3\times10^6$ training iterations, a batch
size of $128$ training samples and a learning rate of $10^{-4}$ was sufficient.
We used a total of $10^6$ training samples in order to adequately cover the
\ac{BBH} parameter space.  We additionally ensure that an (effectively)
infinite number of noise realizations are employed by making sure that every
time a training sample is used it is given a unique noise realisation despite
only having a finite number of waveforms. Each neural network is three layers
deep and has $2048$ neurons in each layer, has a latent space dimension of $64$. 
Training is considered complete when
both components to the loss function have converged to approximately constant
values.

%
% Test procedure
%
After training has completed and we wish to use the network for inference we
follow the procedure described in the right hand panel of
Fig.~\ref{fig:network_config}. Given a new $y$ data sample (not taken from the
training set) we simply input this into the encoder $\textrm{E}_1$ from which we
obtain a single value of $\mu_{1}$ describing a distribution (conditional on the
data $y$) in the latent space. From the multivariate Gaussian distribution
defined by $\mu_{1}$ a random sample from the latent space is generated which,
together with the original input test data $y$, is input into the decoder
network D. The output of the decoder network defines the parameters of a
multivariate Gaussian distribution in the physical parameter space from which a
random sample is drawn. This sample is a random draw from the $r_{\theta}(x|y)$
distribution which represents our approximation to the true posterior. A
comprehensive representation in the form of samples drawn from the entire joint
posterior distribution can then be obtained by simply repeating this procedure
with the same input data (see Eq.~\ref{eq:latent_model}). A more detailed
description is given in Sec.~\ref{sec:methods}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% results - here you would outline the process of comparison between the
% standard approach and the new one. Define training and test data and how Bilby
% is run on all test data for comparison. How do we then train our network. How
% do we then produce results on the test data. Here you refer to results plots
% but try to not make conclusion statememnts (just descriptive). Also include the
% speed analysis here.
%
%
% Intro to the results - what are we trying to do?
%
We present results on $256$ \ac{GW} test waveforms in simulated advanced detector noise and compare
between the existing Bayesian approaches and the \ac{CVAE}. Posteriors produced
by the \texttt{Bilby} inference library~\cite{1811.02042} are used as a
benchmark in order to assess the efficiency and quality of our machine learning
approach with the existing most optimal method for posterior sampling.

%
% describe the Bilby analysis 
%
For the benchmark analysis we assume that 5 parameters are unknown and use a
uniform prior on both black hole masses $m_1$ and $m_2$, which we allow to
range from $35$---$80$ solar masses, a uniform phase ($\phi_0$) prior spanning
$0$---$2\pi$ radians, a uniform distance ($d_{\text{L}})$ prior
%~\chris{we might
%want to change this to be a $d$-squared prior to demonstrate the application of
%priors in the \texttt{VItamin} training stage.} 
from $1$---$3$~Gpc, and a time of
coalescence ($t_{\text{c}}$) prior ranging from
$0.65$---$0.85$~seconds from the
start of the segment. The remainder of the parameters (sky position, binary
inclination angle, polarisation angle, and black hole spins) are all fixed at
constant values. We use timeseries durations of $1$~second with a sampling
frequency of $256$~Hz and the waveform model used is
\texttt{IMRPhenomPv2}~\cite{1809.10113} with a minimum cutoff frequency of
20Hz. For each input test waveform we run the benchmark analysis using multiple
sampling algorithms available within \texttt{Bilby} and for each algorithm we
run twice with different sampling random seeds. For each run and sampler we
extract $5000$ samples from the posterior on the 5 physical parameters.  

%
% the VItamin process
%
The \ac{CVAE} training process used as input $10^{6}$ waveforms corresponding
to parameters drawn from the same priors as assumed for the benchmark analysis.
The waveforms are also of identical duration, sampling frequency, and waveform
model and when each waveform is placed within a training batch it is given a
unique detector noise realisation. The data are then whitened using the same
advanced detector \ac{PSD}~\cite{2016LRR....19....1A} from which the simulated
noise is generated. The \ac{CVAE} posterior results are produced by passing our
256 whitened noisy testing set of \ac{GW} waveforms as input into the testing
path of the pre-trained \ac{CVAE}. For each input waveform we sample until we
have generated $5000$ posterior samples on 4 physical parameters
($x=(m_1,m_2,d_{\text{L}},t_{0})$). We choose to output a subset of the full
5-dimensional space to demonstrate that nuisance parameters (such as $\phi_0$)
can (if desired) be marginalized out within the \ac{CVAE} procedure itself,
rather than after training. 

%
% 1-D overlap results
%
\begin{figure*}
    \includegraphics[width=\textwidth]{images/corner_testcase0.png}
    \caption{\label{fig:corner_plot} Corner plot showing 2 and 1-dimensional
marginalised posterior distributions for one example test dataset. Filled (red)
contours represent the posteriors obtained form the \ac{CVAE} approach and
solid (blue) contours are the posteriors output from our benchmark analysis
(\texttt{Bilby} using the dynesty sampler). In each case the contour boundaries
enclose $68,90$ and $95\%$ probability. One dimensional histograms of the
posterior distribution for each parameter from both methods are plotted along
the diagonal. Blue and red vertical lines represent the $5$---$95\%$
non-symmetric confidence bounds for \texttt{Bilby} and variational inference
respectively. Black crosses and vertical lines denote the true parameter values
of the simulated signal.
} 
\end{figure*}

%
% discuss the corner plot results
%
We can immediately illustrate the accuracy of our machine learning predictions
by directly plotting posteriors (generated using the samples) generated by our
\ac{CVAE} and \texttt{Bilby} approaches superimposed on each other. We show
this for one example test dataset in Fig.~\ref{fig:corner_plot}, where the
strong agreement between both \texttt{Bilby} and the \ac{CVAE} can be clearly
seen. We provide additional test case example corner plots in the
methods section. 
%\hunter{Add more corner plots in methods
%section}~\chris{Maybe, but more corner plots aren't going to convince people.
%We need to agregate the results into a single figure somehow (like the KL
%values). I think something based on the 1-D intervals would be nice.}

%
% K-L divergence results
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/hist-kl.png}
    \caption{\label{fig:kl_results} We plot the KL-divergence values of
five independent samplers against each
other over the same 256 \ac{GW} test waveforms. Posterior samples are computed
for each of the 256 \ac{GW} waveforms using each sampler twice. A KL
divergence is then computed between all samplers with every other sampler
(including themselves) over all 256 \ac{GW} test cases. The distributions of the
resulting KL divergence values are then plotted, with each color representing a
different sampler combination. Both the $x$ and $y$ axes are scaled
logarithmically for readability.} 
\end{figure}
%

%
% discuss the KL results
%
Each independent sampler (including \ac{CVAE}) is run twice on the same test
data to produce samples from the corresponding posterior. We then compute the
KL-divergence between output distributions from each sampler with itself and
each sampler with all other samplers. For distributions that are identical the
KL-divergence is equal to zero but since we are representing our posterior
distributions using finite numbers of samples, identical distributions should
result in KL-divergence values $<1$. In Fig.~\ref{fig:kl_results} we show the
distributions of these KL-divergences for the 256 test \ac{GW} samples where
we see that samplers compared with themselves produce KL values that are
generally $<1$ with the exception of the Emcee sampler. More importantly we see
that the \ac{CVAE} approach when compared to the existing samplers have KL
values that match the values produced when comparing between 2 different
existing samplers.

%
% P-P plot
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/latest_pp_plot.png}
    \caption{\label{fig:pp_plot} One dimensional P-P plot 
    (defined in the main text) using 256
unique test samples and $5000$ posterior sample predictions per test sample.
The $x$-axis denotes the theoretical cumulative distribution whereas the y-axis
denotes the predicted cumulative distribution.
%~\chris{we might have to work on
%the description of the axes - maybe use the same notation that has been used in
%the GW PE papers that include p-p plots.
}
\end{figure}
%

%
% discuss pp plot result
%
%\chris{You need to better describe what a P-P plot is and why we use it. Maybe
%throw in a reference to existing PE group papers that use it. Nature readers
%will have never seen one before. We also have to wait until we get better P-P
%plot curves for our approach before we warrant adding it to the paper.} 
In order to show that our results are consistent with the truth, we have
additionally plotted probability-probability (p-p) plots in
Fig.~\ref{fig:pp_plot}. On the x axis is plotted the theoretical distribution, 
defined as the probability contained within a credibility interval associated 
with the truth. 
On the y axis is plotted the predicted cumulative
distribution, defined as the fraction of predicted values contained within 
the credibility interval defined by the truth \cite{1409.7215}.
Predictions from the \ac{CVAE} consistent with the black dashed diagonal line
is indicative that the Bayesian probability distributions are consistent with
the frequentest interpretation - that the truth will lie inside the $X\%$
probability contour with a frequency of $X\%$ of the time.  We plot empirical
results as those represented in the blue line. 

%
% discuss the speed of the analysis
%
The speed at which posterior samples are generated for all samplers used,
including \texttt{VItamin}, is shown in Table~\ref{Tab:speed}. Run-time is
defined as the total time (in seconds) to produce $\sim 5000$ samples for each
sampler. The dominating computational cost of running \texttt{VItamin} lies in
the training time, which can take of order several hours to complete. 
Completion is determined by comparing posteriors produced 
by the machine learning model and those of \texttt{Bilby} iteratively 
during training. We 
additionally look to see whether the loss curves (Fig. \ref{fig:loss_log}) have flattened out. We use a single Tesla 
V-100 GPU with $16$ Gb of RAM on an NVIDIA DGX-1 machine 
at the LIGO Livingston site.
Once trained there is no need to retrain the network and it may be run
on any new input data of the same format. Reliable posterior estimates can then
be expected if the data contain signals whose true parameters lie within the
prior space over which the network has been trained and whose noise
characteristics are consistent with that used in training. For our test case of
\ac{BBH} signals \texttt{VItamin} produces samples from the posterior at a rate
which is $\sim 6$---$7$ orders of magnitude faster than current inference
techniques, representing a dramatic speed-up in performance. It should be noted
that run-time for the Bayesian samplers is also dependent on the choice of
sampler hyper-parameters, so total run-time may vary according to specific
configuration choices. We choose Bayesian sampler hyper-parameters based on
standard default configurations defined in the \texttt{Bilby}
documentation~\cite{1811.02042}. 
 

%\chris{One more thing to add, especially if the other KL, AD, and PP plots
%aren't convincing, is a plot displaying 1D confidence bounds compared between
%bilby and VItamin. Imagine a plot with the x-axis as distance and the y-axis
%steps through test data with increasing true distance. for each test data you
%plot 2 error bars horizontally (one for bilby and one for VItamin) spanning the
%range of 90\% confidence. You would hopefully get nearly identical pairs of
%errorbars stacked vertically. Technically you could do this for all parameters
%(and you should) but we might only put one of the plots in the paper (if at
%all).}

%
% I feel the need, the need for speed, table
% 
\begin{table}
\centering
\caption{Durations required to produce $5000$ posterior samples from each of
the different posterior sampling approaches}
\begin{tabular}[t]{lcccc}
\toprule
& & run time (sec) & & \\
& min & max & median & Ratio \\
\hline
Dynesty & 602 & 1538 & 774 & $2.6\times 10^{-6}$ \\
Emcee & 2005 & 11927 & 4351 & $4.6\times 10^{-7}$ \\
Ptemcee & 3354 & 12771 & 4982 & $4.0\times 10^{-7}$ \\
Cpnest & 1431 & 5405 & 2287 & $8.8\times 10^{-7}$ \\
VItamin & - & - & \bm{$2\times 10^{-3}$} & 1 \\
\botrule
\end{tabular}
\label{Tab:speed}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% conclusions - now draw conclusions about the quality of the comparison
% results. Highlight the current limitations but also highlight the importance of
% this for the GW field (multi-detector is easy, additional parameters are easy,
% longer datasets may be a challenge regarding GPU memory?, we don't have to
% assume a noise model if we inject training data into real noise, we do rely on
% well defined signal models, EM-follow up in very low latency, can we use
% transfer learning if we want to retrain, ...) End with broader statements about
% inference in other fields and how this is applicable across the sciences.
%
% recap and main result
%
There are currently a variety of challenges within the field of \ac{GW}
parameter estimation including, and not limited to, accounting for detector
artefacts (non-Gaussian transient "glitches") in the data, time-dependent
variations (non-stationarity) of the detector noise power spectrum density,
systematic uncertainties in the waveform models themselves and decreasing the
latency with which parameter estimates are produced~\cite{1409.7215}. In this
letter we have demonstrated that we are able to reproduce, to a high degree of
accuracy, Bayesian posterior probability distributions generated through
machine learning. This is accomplished using \acp{CVAE} trained on simulated
\ac{GW} signals and does not require the input of precomputed posterior
estimates. We have demonstrated that our neural network model which, when
trained, can reproduce complete and accurate posterior estimates in
$\mathcal{O}(1)$ milliscond, can achieve the same quality of results as the
trusted benchmark analyses used within the LIGO-Virgo Collaboration.

%
% discuss the near term CBC implications and why this is a game-changer
%
The significance of our results is most evident in the orders of magnitude
increase in speed over existing approaches. This increase will help the
LIGO-Virgo collaboration alert \ac{EM} follow-up partners with minimum latency.
Improved low-latency alerts will be especially pertinent for signals from
\ac{BNS}, like GW170817 \cite{PhysRevLett.119.161101}, and \ac{NSBH} signals where
parameter estimation will no longer be limiting factor\footnote{The complete
low-latency pipeline includes a number of steps. The process of \ac{GW} data
aquisition is followed by the transfer of data. There is then the corresponding
analysis and the subsequent communication of results to the \ac{EM} astronomy
community after which there are physical aspects such as slewing observing
instruments to the correct pointing.} in observing the prompt \ac{EM} emission
expected on shorter time scales than is acheivable with existing \ac{LVC}
analysis tools such as Bayestar~\cite{2016PhRvD..93b4013S}. The predicted number of
future detections of \ac{BNS} mergers ($\sim 180$) will severely
strain the \ac{GW} cummunity's current computational resources using existing
Bayesian methods. Our approach will provide (potentially) full-parameter
estimation on \ac{CBC} signals in seconds on a single \ac{GPU}. Our trained
network is also incredibly modular, and can be shared and used easily by any
user to produce results. The specific analysis described in the letter assumes
a uniform prior on the signal parameters but this is a choice and the network
can be trained with any prior the user demands. However, posterior samples
obtained using a network trained on the uniform prior are essentially samples
from the likelihood. Users then have the option to (cheaply) resample from the
posterior according to their own prior choices. We also note that our method
will be invaluable for population studies since populations may now be
generated and analysed in a full-Bayesian manner on a vastly reduced and
therefore tractable time scale. 

%
% future work, current limitations and prospects
%
\ac{GW} data is typically sampled at 16kHz and downsampled to more managable
rates dependent on the signal class being analysed. For \ac{BBH} signals this
is usually chosen as $1$---$4$ KHz dependent upon mass of binary wheras we have
chosen to use the noticably low sampling rate of 256Hz largely in order to
decrease the computational time required to develop our approach. We do not
anticipate any problems in extending our analysis to higher sampling
frequencies other than an increase in training time and a larger burden on the
\ac{GPU} memory. The lower sampling rate naturally limited our chosen \ac{BBH}
mass parameter space to high mass signals. We similarly do not anticipate that
extending the parameter space to lower masses will lead to problems but do
expect that a larger number of training samples may be required. Future work
will incorporate a multi-detector configuration at which point parameter
estimation can be extended to sky localisation. In this work we have elected to
use a single detector configuration so that we may again decrease the
computational time required to develop the approach.

%
% Non gaussian noise and the final statement
%
In reality, \ac{GW} detectors are affected by non-Gaussian noise artefacts. To
account for this noise within our scheme, we would train our network using
samples of real detector noise (preferably recent examples to best reflect the
state of the detectors). Our method has the added advantage of not being
dependent on the choice of \ac{PSD} used for whitening, unlike current Bayesian
methods. The whitening procedure applied to the training data for the \ac{CVAE}
is simply to rescale the input to a scale more suitable to neural networks
whereas for existing methods assumptions on the \ac{PSD} directly affect the
posterior results. Our work can naturally be expanded to include the full range
of \ac{CBC} signal types but also to any and all other parameterised \ac{GW}
signals and to analyses of \ac{GW} data beyond that of ground based
experiments. Given the abundant benefits of this method, we hope that a variant
of this of approach will form the basis for all \ac{GW} parameter estimation
over the next several decades to come.

%
% acknowledge people and funding agencies
%
\section{Acknowledgements.}
%
We would like to acknowledge valuable input from the LIGO-Virgo Collaboration,
specifically from the parameter estimation and machine-learning working groups.
We thank Nvidia for the generous donation of a Tesla V-100 GPU used in addition
to \ac{LVC} computational resources. The authors also gratefully
acknowledge the Science and Technology Facilities Council of the United
Kingdom. CM and SH are supported by the Science and Technology Research Council
(grant No.~ST/~L000946/1) and the European Cooperation in Science and
Technology (COST) action CA17137.

%% Here is the endmatter stuff: Supplementary Info, etc.
%% Use \item's to separate, default label is "Acknowledgements"

\section{addendum}
 \subsection{Competing Interests} 
    The authors declare that they have no competing financial interests.
 \subsection{Correspondence} Correspondence and requests for materials should be addressed to Hunter Gabbard~(email: h.gabbard.1@research.gla.ac.uk).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% methods - Everything that we couldn't fit in. Mostly validation plots.
%
\section{Methods}\label{sec:methods}
%
%Put methods in here.  If you are going to subsection it, use
%\verb|\subsection| commands.  Methods section should be less than
%800 words and if it is less than 200 words, it can be incorporated
%into the main text.

%
% A description of the loss function derivation 
%
We begin with the statement defining the aim of the analysis. We wish to obtain
a function that reproduces the posterior distribution (the probability of our
physical parameters $x$ given some measured data $y$). The cross entropy
between 2 distributions is defined as
%
\begin{equation}\label{eq:cross_ent}
H(p,r) = -\int dx\, p(x|y) \log r_{\theta}(x|y)
\end{equation}
%
where we have made the distributions explicitly conditional on $y$ (our
measurement). In this case $p(x|y)$ is the target distribution (the true
posterior) and $r_{\theta}(x|y)$ is the parametric distribution that we will
use neural networks to construct. In this case $\theta$ represents the
trainable neural network parameters. 

The cross-entropy is minimised when $p(x|y)=r_{\theta}(x|y)$ and so by
minimising
%
\begin{align}\label{eq:cost1}
H &= -\text{E}_{p(y)}\left[\int dx\,p(x|y) \log r_{\theta}(x|y)\right]
\end{align}
% 
where $\text{E}_{p(y)}[\cdot]$ indicates the expectation value over the
distribution of measurements $y$, we therefore make the parameteric distribution
as similar to the target for all possible measurements $y$.

Converting the expectation value into an integral over $y$ weighted by $p(y)$
and applying Bayes theorem we obtain
%
\begin{align}\label{eq:cost1}
H &= -\int dx\,p(x)\int dy\,p(y|x)\log r_{\theta}(x|y)
\end{align}
%
where $p(x)$ is the prior distribution on the physical parameters $x$.

The conditional variational autoencoder network outlined in
Fig.~\ref{fig:network_config} makes use of a conditional latent variable model.
Our parameteric model is constructed from the product of 2 seperate
distributions marginalised over the latent space
%
\begin{align}\label{eq:latent_model}
r_{\theta}(x|y) &= \int dz\,r_{\theta_{1}}(z|y)r_{\theta_{2}}(x|z,y).
\end{align}
%  
We have used $\theta_{1}$ and $\theta_{2}$ to indidate that the 2 seperate
networks modelling these distributions will be trained on these parameter sets
respectively. Both new conditional distributions are modelled as $n_{z}$
dimensional multivariate uncorrelated Gaussian distributions (governed by their
means and variances). However, this still allows $r_{\theta}(x|y)$ to take a
general form (although it does limit it to be unimodal).  

One could be forgiven in thinking that by setting up networks that simply aim
to minimise $H$ over the $\theta_{1}$ and $\theta_{2}$ would be enough to solve
this problem. However, as shown in~\cite{NIPS2015_5775} this is an intractable
problem and a network cannot be trained directly to do this. Instead we define
a recognition function $q_{\phi}(z|x,y)$ that will be used to derive an
\ac{ELBO}.

Let us first define the \ac{KL} divergence between 2 of our
distributions as
%
\begin{align}\label{eq:kl}
\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{2}}(z|x,y)\right] &= \int dz\,
\log\left(\frac{q_{\phi}(z|x,y)}{r_{\theta_{2}}(z|x,y)}\right).
\end{align}
%  
It can be shown that after some manipulation that
%
\begin{align}\label{eq:elbo1}
\log r_{\theta}(x|y) &= L + \text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{?}}(z|x,y)\right]
\end{align}
%
where the \ac{ELBO} $L$ is given by
%
\begin{align}\label{eq:elbo2}
L &= \int dz\,
q_{\phi}(z|x,y)\log\left(\frac{r_{\theta_{2}}(x|z,y)r_{\theta_{1}}(z|y)}{q_{\phi}(z|x,y)}\right)
\end{align}
%
and is so-named since $\text{KL}$ cannot be negative and has a minimum of zero.
Therefore, if we were to find a $q_{\phi}(z|x,y)$ function (optimised on
$\phi$) that minimised the \ac{KL}-divergence then we can state that
%
\begin{align}
\log r_{\theta}(x|y) &\geq L.
\end{align}
%
After some further manipulation of Eq.~\ref{eq:elbo2} we find that
%
\begin{align}\label{eq:logr}
\log r_{\theta}(x|y) &\geq  \text{E}_{q_{\phi}(z|x,y)}\left[\log
r_{\theta_{2}}(x|z,y)\right] \nonumber\\
&-\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{1}}(z|y)\right].
\end{align}
%
We can now substitute this inequality into Eq.~\ref{eq:cost1} (our cost
function) to obtain
%
\begin{align}\label{eq:cost2}
H \leq & -\int dx\, p(x)\int dy\,p(y|x)
\left\{\text{E}_{q_{\phi}(z|x,y)}\left[\log r_{\theta_{2}}(x|z,y)\right]
\right.\nonumber\\
&-\left.\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{1}}(z|y)\right]\right\}  
\end{align}
%
which can in practice be approximated as a stochastic integral over draws of
$x$ from the prior, $y$ from the likelihood function $p(y|x)$, and from the
recognition function, giving us Eq.~\ref{eq:cost3}.

In this case we draw $N$ sets of physical parameter, corresponding data
sets, and draws from the recognition function. It is important to note that
whilst it is true that the \ac{KL}-divergence between the $q_{\phi}(z|x,y)$ and
$r_{\theta_{2}}(z|x,y)$ (seen in Eq.~\ref{eq:elbo1}) should be minimised and in
an ideal case be equal to zero, that is not the case for the \ac{KL}-divergence in
Eq.~\ref{eq:cost3}. In this case our aim is to minimise $H$ which implies that
the \ac{KL}-divergence should be low but not necessarily zero.

We have now set up a system composed of 3 functions that have well defined
inputs and outputs where the mapping of those inputs to outputs is governed by
the parameter sets $\theta_{1},\theta_{2},\phi$. These parameters are the
weights and biases of 3 neural networks acting as (variational) encoder,
decoder, and encoder respectively. To train such a network one must connect the
inputs and outputs appropriately to compute the cost function $H$ and
back-propogate cost function derivatives to update the network parameters. The
network structure shown in Fig.~\ref{fig:network_config} shows how for a batch
of $N$ sets of $x$ and corresponding $y$ values first both $x$ and $y$ are fed
into the encoder 2 network that produces the means and variances of the latent
space $z$ describing the $q_{\phi}(z|x,y)$ distribution function. Next (the
order does not matter) the data $y$ alone is passed into the encoder 1 network
that produces the means and variances of the latent space $z$ describing the
$r_{\theta_{1}}(z|y)$ distribution function. Given the parameters of these
Gaussian (diagonal covariance matrix) distributions the $\text{KL}$-divergence
is computed as
%
\begin{align}\label{eq:klgauss}
\text{KL}&\left[q_{\phi}(z|x_{n},y_{n})||r_{\theta_{1}}(z|y_{n})\right] =
\\
&\frac{1}{2}\sum_{j=1}^{n_{z}}\left[\frac{\sigma_{q,j}^{2}}{\sigma_{r,j}^{2}} +
\frac{(\mu_{r,j}-\mu_{q,j})^{2}}{\sigma_{r,j}^{2}}+
\log\left(\frac{\sigma_{r,j}^{2}}{\sigma_{q,j}^{2}}\right)\right] -
\frac{n_{z}}{2}\nonumber 
\end{align}
%
where those labelled and indexed parameters output from the encoder networks are
denoted by $\mu$ and $\sigma$.

To evaluate the first term within the cost function (Eq.~\ref{eq:cost3}) we
require the data $y$ and a randomly drawn $z$ value from the latent space defined
by $q_{\phi}(z|x,y)$. These are then passed as input into the decoder network D
that produces the means and variances in the physical parameter space $x$ describing
the $r_{\theta_{2}}(x|z,y)$ distribution function. With the function now well
defined we simply evaluate the function at the values of the input $x_{n}$
values. Training of the network proceeds until a convergence of the cost is
seen. 

%
% loss plot
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/inv_losses_log.png}
\caption{\label{fig:loss_log} We plot here three loss curves; the cost
function loss (blue), KL (orange) and total loss (green). The cost  loss and KL
are both derived in the methods section and the total loss is simply a
summation of the two. We plot the value of each loss as a function of the log
of the number of training iterations where one iteration is defined as a
training session over one batch of signals.}
\end{figure}

When the network has been trained it can be used for the inference of unknown
physical parameters given a new single instance of measured data $y$. To do
this one is interested in obtaining samples from the distribution
$r_{\theta}(x|y)$ which can be obtained by evaluating
Eq.~\ref{eq:latent_model}. To do this using the network we first obtain the
parameters governing the function $r_{\theta_{1}}(z|y)$ by passing the data $y$
into the encoder 1 network. Using the output means and variances on the latent space
$z$ we then draw from that space to obtain a random latent space realisation.
This $z$ value and the original $y$ data is then input into the decoder network
which outputs the mean and variance parameters governing the function
$r_{\theta_{2}}(x|z,y)$. The next step is to draw a random $x$ realisation from
that function. This is representative of a posterior sample from the target
distribution $p(x|y)$ and to obtain an informative description of the entire
distribution one would repeat the procedure with the same input data
$\mathcal{O}(1000)$ times from which parameter confidence bounds can be
determined.      

%
% Training procedure
%
Training is performed via a series of steps illustrated in
Fig.~\ref{fig:network_config}.
%
\begin{itemize}
%
\item The encoder $\textrm{E}_1$ is given a set of training \ac{GW} signals
($y$) and encodes $y$ into a set of variables $\mu^{j}_{1}$ defining a
distribution in the latent space. In this case $\mu^{j}_{1}$ describes the
first 2 central moments for each dimension of a multivariate Gaussian where $j$
is the order of the moment ($j=1$ is the mean and $j=2$ is the variance).
%
\item The encoder $\textrm{E}_2$ takes a combination of both the data $y$ and
the true parameters $x$ defining the \ac{GW} signal and encodes this into
parameters defining another multivariate Gaussian distribution in the same
latent space. These parameters we denote by $\mu^{j}_{2}$.
%
\item We then sample from the distribution described by $\mu^{j}_{2}$
giving us samples $z_{2}$ within the latent space.
%
\item These samples, along with their corresponding $y$ data, then go to the
decoder D which outputs $\mu_{x}$, a set of parameters (much like
$\mu^{j}_{1},\mu^{j}_{2}$) that define the moments of a multivariate Gaussian
ditribution in the $x$ space.
 %
\item The first term of the loss function (Eq.~\ref{eq:cost3}) is then computed
by evaluating the probability density defined by $\mu_x$ at the true $x$
training values. The component of the loss allows the network to learn how to
predict accurate values of $x$ but to also learn the intrinsic variation due to
the noise properties of the data $y$. It is important to highlight that the
\ac{GW} parameter predictions from the decoder D do describe a multivariate
Gaussian but, as we will show, this does \emph{not} imply that our final output
posterior estimates will also be multivariate Gaussians.
%
\item Finally the loss component described by the K-L divergence between the
distributions described by $\mu^{j}_1$ and $\mu^{j}_2$ is computed. For further
details on training and the full derivation of the loss functions, see methods
subsection \textit{Loss function derivation}. Here we highlight that we do not
desire that the network tries to make these 2 distributions equal to each
other. Rather, we want the ensemble network to minimise the total loss (of
which this is a component).
%
\end{itemize}

We then repeat the following 3 steps to
obtain new samples from the posterior

%~\chris{Hunter, note that we only need to
%do the first pass through E1 only once for a given peice of data do the real
%cost is only in the other steps. This will likely only change the timing by
%~25\% so it might not be worth doing the tests again. Just bear it in mind.}
%
\begin{itemize}
%
\item We randomly draw a latent space sample $z_1$.
%
\item Our $z_1$ sample and the corresponding $y$ data are fed as input to our
pre-trained decoder network. The decoder network returns a set of moments
$\mu^{x}_1$ which describe a multivariate Gaussian distribution in the physical
parameter space.
%
\item We then draw a random $x$ realisation from that distribution.
%
\end{itemize}
%


\bibliographystyle{apsrev4-1}
\bibliography{references}% Produces the bibliography via BibTeX.

\end{document}
