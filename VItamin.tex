%% Template for a preprint Letter or Article for submission
%% to the journal Nature.
%%

\documentclass[%
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
showpacs,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
 twocolumn,
 prl,
 reprint,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage{lineno}
\usepackage{color}
\usepackage{acronym}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{hyperref}
%\usepackage{booktabs}

\hypersetup{
%--- fill inside borders ---
  colorlinks=true,        % false: boxed links; true: colored links
  linkcolor=black,         % color of internal links
  citecolor=cyan,         % color of links to bibliography
}

%% ----- comment commands for each of us
\newcommand{\chris}[1]{\textbf{\textcolor{red}{CHRIS: #1}}}
\newcommand{\francesco}[1]{\textbf{\textcolor{green}{FRANCESCO: #1}}}
\newcommand{\hunter}[1]{\textbf{\textcolor{blue}{HUNTER: #1}}}
\newcommand{\siong}[1]{\textbf{\textcolor{cyan}{SIONG: #1}}}
\newcommand{\rod}[1]{\textbf{\textcolor{yellow}{ROD: #1}}}

\begin{document}

\preprint{APS/123-QED}

\title{Estimating Bayesian parameter estimation using conditional variational
autoencoders for gravitational-wave astronomy}

\author{Hunter Gabbard$^1$}
 \email{Corresponding author: h.gabbard.1@research.gla.ac.uk}
\author{Chris Messenger$^1$}
\author{Ik Siong Heng$^1$}
\author{Francesco Tonolini$^2$}
\author{\& Roderick Murray-Smith$^2$}

\affiliation{
 SUPA, School of Physics and Astronomy$^1$, \\
 University of Glasgow, \\
 Glasgow G12 8QQ, United Kingdom \\ \\
 School of Computing Science$^2$, \\
 University of Glasgow, \\
 Glasgow G12 8QQ, United Kingdom \\
}

\date{\today}

\maketitle

\acrodef{GW}[GW]{gravitational wave}
\acrodef{BBH}[BBH]{binary black hole}
\acrodef{EM}[EM]{electromagnetic}
\acrodef{BNS}[BNS]{binary neutron star}
\acrodef{SNR}[SNR]{signal-to-noise ratio}
\acrodef{PSD}[PSD]{power spectral density}
\acrodef{FFT}[FFT]{fast Fourier transform}
\acrodef{CNN}[CNN]{convolutional neural network}
\acrodef{ROC}[ROC]{receiver operator characteristic}
\acrodef{ELBO}[ELBO]{evidence lower bound}
\acrodef{LIGO}[LIGO]{advanced Laser Interferometer Gravitational wave Observatory}
\acrodef{CVAE}[CVAE]{conditional variational autoencoder}
\acrodef{AD}[AD]{Anderson-Darling}
\acrodef{KS}[KS]{Kolmogorov-Smirnoff}
\acrodef{KL}[KL]{Kullback–Leibler}
\acrodef{GPU}[GPU]{graphics processing unit}

%
% Introductory paragraph describing the content of the letter
%
% This format begins with a title of, at most, 15 words, followed by an
% introductory paragraph (not abstract) of approximately 150 words, summarizing
% the background, rationale, main results (introduced by "Here we show" or some
% equivalent phrase) and implications of the study. This paragraph should be
% referenced, as in Nature style, and should be considered part of the main
% text, so that any subsequent introductory material avoids too much redundancy
% with the introductory paragraph.
%
\textbf{ 
%
% background
%
With the beginning of the \ac{LIGO} and Virgo's third observation run well
under way, we are now in an era where \ac{GW} detection is
commonplace~\cite{PhysRevLett.116.061102,
PhysRevX.6.041015,PhysRevLett.119.161101}. As the sensitivity of the global
network of gravitational wave detectors increases, we will observe
$\mathcal{O}(100)$s of transient \ac{GW} events per year~\cite{1409.7215}. For
each of these events the current methods used to estimate their source
parameters employ computationally costly Bayesian inference
approaches~\cite{1409.7215}.
%
% rationale
%
Bayesian strategies are known to be optimal for \ac{GW} parameter
estimation~\cite{Searle_paper}. However, when applied to timeseries containing
a single \ac{BBH} event sampled at 4kHz, existing \ac{GW} analyses can take
$\mathcal{O}(1.5\times 10^{5} - 1.7\times 10^{6}\:\textrm{seconds})$ to
complete~\cite{1409.7215}. For \ac{BNS} signals this timescale increases by at
least an order of magnitude ~\cite{PhysRevLett.119.161101} and it is for this class of
signal that counterpart \ac{EM} signatures are expected, containing crucially
Gamma and X-ray information on timescales of 1 second -- 1 minute. The \ac{GW}
community strives towards alerting their \ac{EM} follow-up partners in a timely
and efficient manner and the current fastest method for doing so,
\texttt{Bayestar}, can provide alerts in $\mathcal{O}(1\: \textrm{minute})$,
and only approximate parameter estimates on a limited range of source
parameters. Given the deluge of expected signals, the large computational cost
and delay in obtaining complete source information, together with the need for
alerts to \ac{EM} partners on $<1$ second timescales, it is imperative that a
significantly more efficient parameter estimation method be developed. We
propose the use of a \ac{CVAE}~\cite{1904.06264,1812.04405} as a rapid and
accurate alternative to this approach. 
%
% results
%
Here we show that a \ac{CVAE} pre-trained on $\sim 10^{6}$ \ac{BBH} signals and
without being given the precomputed posteriors can return Bayesian posterior
probability estimates on 4 of the source parameters (easily scalable to higher
dimensional problems). The training procedure takes $\sim 1$ day on a single
\ac{GPU} but need only be performed once for a given prior parameter space (or
signal class). The resulting trained machine can then produce single
(statistically independent) posterior samples in $\sim 0.2$ microseconds making
this approach $\sim 6$ orders of magnitude faster than existing techniques and
therefore allowing full Bayesian parameter estimation in well under 1 second
for \ac{BBH} signals.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% introduction - this section has to expand upon what has mentioned in the
% abstract background (which was only ~50 words). It needs to cover the state of
% the gravitational wave field and the number of detections expected in the next
% ~5 years. It should briefly discuss the issue of low latency EM follow up. It
% needs to cover Bayesian inference (not in too much detail) and the signal model
% we are interested in here (again, not too much detail but enough for the
% average Nature reader). It then needs to introduce machine learning and focus
% mainly on how our scheme works. We also need to include a statement about how
% the training data priors affect the result (are they really the priors?)
%
% Intro to the detection era with the LVC
%
With the overwhelmingly successful observation runs of O1 and O2 now complete,
\ac{LIGO} and Virgo have produced a large catalogue of \ac{GW} data covering
both \ac{BBH} and \ac{BNS} signals\cite{1811.12907}. Over the next five years
we expect the number of detections to increase to be upwards of $\sim180$
\ac{BNS} and $\sim400$ BBH events per year~\cite{1304.0670,1811.12907}. This
large influx in the number of detections will put an increased amount of
pressure on the current \ac{GW} inference methods used for parameter
estimation.  

%
% From GW detection, to parameter estimation
%
Much of the \ac{LIGO} analysis effort over the past several years has been
focused on the detection of \ac{GW}s. This has been the primary focus of many,
due to the difficulty associated with identifying \ac{GW} waveforms buried in
in large amount of noise to a high degree of certainty. The detection problem
has largely been solved through the use of matched template
filtering~\cite{0264-9381-33-21-215004}. Once a \ac{GW} has been identified
through matched template filtering, Bayesian inference is used to extract
information about the source parameters of the detected \ac{GW} event.

%
% Set up parameter estimation problem
%
In the standard Bayesian \ac{GW} inference approach, we assume that we are
given both a signal model and a noise model. Both the signal and the 
noise model may have unknown parameters we are interested in inferring. 
Each parameter is given a prior astrophysically motivated probability 
distribution. In our case, we have additive noise which is modelled as 
a Gaussian (in reality, the data is not truly Gaussian). Given a noisy
\ac{GW} waveform, we would like to find an optimal procedure for retrieving
some finite set of unknown \ac{GW} parameters. Our procedure should be able
to give us an accurate estimate of the parameters of our observed signal, while
also accounting for the uncertainty which arises from having multiple noise
realizations of our observed data able to be mapped to one parameter
estimate.

%
% Describe Bayes Theorem
%
According to Bayes Theorem, a posterior for a set of parameters can be
represented as
%
\begin{align}\label{eq:bayes_theorem}
    p(x|y) &\propto p(y|x) p(x),
\end{align}
%
where $x$ are the parameters, $y$ is the observed data, $p(x|y)$ is the
posterior, $p(y|x)$ is the likelihood, and $p(x)$ is the prior we put on our
parameter distribution. The constant of proportionality, which we omit here, is
$p(y)$, the probability of our data, known as the Bayesian evidence or the
marginal likelihood. We typically ignore $p(y)$ since it is a constant and for
parameter estimation purposes we are only interested in the shape of the
posterior.

Due to the size of the parameter space typically encountered in \ac{GW} 
parameter estimation and the large quantities of data used, we must 
stochastically sample the parameter space in order to estimate 
the posterior. Sampling is done using a 
variety of techniques including nested sampling 
\cite{cpnest,dynesty} and Markov chain Monte Carlo methods \cite{emcee,ptemcee}. The primary software tools used by the \ac{LIGO} 
parameter estimation analysis are \texttt{LALInference} and 
\texttt{Bilby} \cite{1409.7215,1811.02042}, which contain many 
different sampling methods.  
  
%
% Intro to machine learning section
%
Machine learning has featured prominently in many areas of gravitational wave
research over the last few years. These techniques have shown to be
particularly promising in signal detection
\cite{GEORGE201864,PhysRevLett.120.141103,1904.08693}, glitch classification
\cite{1706.07446,0264-9381-34-6-064003} and earthquake prediction
\cite{Coughlin_2017}. Recently, a type of neural network known as \ac{CVAE} was
shown to perform exceptionally well when applied towards computational imaging
inference~\cite{1904.06264,NIPS2015_5775}, text to image
inference~\cite{1512.00570}, high-resolution synthetic image
generation~\cite{1612.00005} and the fitting of incomplete heterogeneous
data~\cite{1807.03653}. It is this type of machine learning network that we
utilize to accuratelly approximate the Bayesian posterior $p(x|y)$, which is
known to be the optimal result ~\cite{0809.2809}.

%
% What is an autoencoder?
%
Conditional variational autoencoders are a form of variational autoencoder
which are conditioned on an observation, where in our case the observation is a
1-dimensional \ac{GW} time series signal $y$. The autoencoders from which
variational autoencoders are derived are typically used for problems involving
image reconstruction and/or dimensionality reduction. They perform a regression
task whereby the autoencoder attempts to predict its own given input (model the
identity function) through a limited and therefore distilled representation of
the input parameter space. An autoencoder is composed of two neural networks,
an encoder and a decoder~\cite{LIOU20083150}. The encoder network takes as
input an vector, where the number of
dimensions is a fixed number predefined by the user. The encoder converts the
input vector into a (typically) lower dimensional space,
referred to as the {\it{latent space}}. A representation of the data in the
latent space is passed to the decoder network which generates a reconstruction
of the original input data to the encoder network. Through training, the two
sub-networks learn a how to efficiently represent a dataset within a lower
dimensional latent space which will take on the most important properties of
our input training data. In this way, the data can be compressed with little
loss of fidelity. Additionally, the decoder simultaneously learns to decode the
latent space representation and reconstruct that data back to its original form
(the input data). 

%
% What is a variational autoencoder?
%
The primary difference between a variational autoencoder~\cite{1812.04405} and
an autoencoder concerns the method by which locations within the latent space
are produced. In our variant of the variational autoender, the output of the
encoder is interpreted as a set of parameters governing statistical
distributions (in our case the means and variances of multivariant Gaussians).
In proceeding to the decoder network, samples are randomly drawn from these
distributions ($z$) and fed into the decoder, therefore adding an element of
variation into the process. Therefore a particular input can have infinitely
many outputs. In both the decoder and the encoder networks we use
fully-connected layers (although this is not a constraint and any trainable
network architecture may be used).

%
% Brief introduction to loss functions used in the neural networks
%
The construction of a \ac{CVAE} begins with the definition of a quantity to be
minimised (referred to as a loss or cost function). We can relate that aim to
that of approximating the posterior distribution by minimising the cross
entropy, defined as
%
\begin{align}\label{eq:cross_ent} H(p,r) &= -\int dx\, p(x|y) \log
r_{\theta}(x|y) \end{align}
%
between the true posterior $p(x|y)$ and $r_{\theta}(x|y)$, the parametric
distribution that we will use neural networks to construct and which we aim to
be equal to the true posterior. In this case $\theta$ represents a set of
trainable neural network parameters. Starting from this point it is possible to
derive a computable form for the cross-entropy that is reliant on a set of
unknown functions that can be modelled by the components of variational encoder
and decoder neural networks. The details of the derivation are described in
Sec.~\ref{sec:methods} and in~\cite{1904.06264}. The final form of the
cross-entropy loss function is given by the bound
%
\begin{align}\label{eq:cost3}
C \geq & \frac{1}{N}\sum_{n=1}^{N}\left[\log
r_{\theta_{2}}(x_{n}|z_{n},y_{n})\right.\nonumber\\
&\left.-\text{KL}\left[q_{\phi}(z|x_{n},y_{n})||r_{\theta_{1}}(z|y_{n})\right]\right].
\end{align}
%
and requires three fully-connected networks; two encoder networks (labelled
$\textrm{E}_1$, $\textrm{E}_2$ in Fig.~\ref{fig:network_config}) representing
the functions $r_{\theta_{1}}(z|y)$ and $q_{\phi}(z|x,y)$ respectively, and one
decoder network (D) representing the function $r_{\theta_{2}}(x|z,y)$. The
function $\text{KL}(\cdot||\cdot)$ denotes the Kulback-Leibler divergence. In
practice, during the training procedure the various integrations that are part
of the derivation are approximated by a sum over $N$ training data samples at
each stage of training.

\begin{figure}
    \includegraphics[width=\columnwidth]{images/network_setup.png}
    \caption{\label{fig:network_config} This figure illustrates our neural
network training/testing operations. During training (left-hand side), 
a training set of noisy
\ac{GW} signals ($y$) and their corresponding true parameters ($x$) are given
as input to encoder network $\textrm{E}_2$, while only $y$ is 
given to $\textrm{E}_1$. The K-L divergence (Eq.~\ref{eq:kl}) is computed
between the moments of latent space representations. We then sample from a
multivariate Gaussian whose mean and variances are defined by $\mu^{j}_{2}$, 
which is given as input to the decoder network. We take the first two
moments $\mu^{j}_x$ of the predictions from the decoder network, along with our
training parameters $x$ and compute a cost function Eq. \ref{eq:cost}. After
having trained our networks, we test (right-hand side) using only $\textrm{E}_1$ and the decoder.
The decoder returns a multi-dimensional Gaussian function that describes 
the likelihood/posterior and so we simply sample from it. A more 
detailed description of the network can be found in the letter text.} 
\end{figure}

%
% Training procedure
%
Training is performed via a series of steps illustrated in
Fig.~\ref{fig:network_config}. 
%
\begin{itemize}
%
\item The encoder $\textrm{E}_1$ is given a set of training \ac{GW} signals
($y$) and encodes $y$ into a set of variables $\mu^{j}_{1}$ defining a
distribution in the latent space. In this case $\mu^{j}_{1}$ describes the
first 2 central moments for each dimension of a multivariate Gaussian where $j$
is the order of the moment ($j=1$ is the mean and $j=2$ is the variance). 
%
\item The encoder $\textrm{E}_2$ takes a combination of both the data $y$ and
the true parameters $x$ defining the \ac{GW} signal and encodes this into
parameters defining another multivariate Gaussian distribution in the same
latent space. These parameters we denote by $\mu^{j}_{2}$.
%
\item We then sample from the distribution described by $\mu^{j}_{2}$
giving us samples $z_{2}$ within the latent space. 
%
\item These samples, along with their corresponding $y$ data, then go to the
decoder D which outputs $\mu_{x}$, a set of parameters (much like
$\mu^{j}_{1},\mu^{j}_{2}$) that define the moments of a multivariate Gaussian
ditribution in the $x$ space.
 %
\item The first term of the loss function (Eq.~\ref{eq:cost3}) is then computed
by evaluating the probability density defined by $\mu_x$ at the true $x$
training values. The component of the loss allows the network to learn how to
predict accurate values of $x$ but to also learn the intrinsic variation due to
the noise properties of the data $y$. It is important to highlight that the
\ac{GW} parameter predictions from the decoder D do describe a multivariate
Gaussian but, as we will show, this does \emph{not} imply that our final output
posterior estimates will also be multivariate Gaussians.
%
\item Finally the loss component described by the K-L divergence between the
distributions described by $\mu^{j}_1$ and $\mu^{j}_2$ is computed. For further
details on training and the full derivation of the loss functions, see methods
subsection \textit{Loss function derivation}. Here we highlight that we do not
desire that the network tries to make these 2 distributions equal to each
other. Rather, we want the ensemble network to minimise the total loss (of
which this is a component).  
%
\end{itemize}

%
% Some practical aspects of the training
%
The losses computed from the K-L divergence and the cost function are summed
and then back-propagated through all three networks (E1, E2, and D).  As is
standard practice in machine learning applications, the loss is computed over a
batch of training samples and repeated for a pre-defined number of iterations.
For our purposes, we found that $\sim3\times10^6$ training iterations, 
a batch size of $128$ training samples and a
learning rate of $10^{-4}$ was sufficient. We used a total of $10^6$ training
samples in order to adequately cover the \ac{BBH} parameter space.  We
additionally ensure that an (effectively) infinite number of noise realizations
are employed by making sure that every time a training sample is used it is
given a unique noise realisation despite only having a finite number of
waveforms. Each neural network is three layers deep, has $2048$ neurons in each
layer, has a latent space dimension of $64$ with $50\%$ dropout applied to each
layer. Training is considered complete when both components to the loss
function have converged to constant values..

%
% loss plot
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/inv_losses_log.png}
\caption{\label{fig:loss_log} We plot here three loss curves; the cost
function loss (blue), KL (orange) and total loss (green). The cost  loss and KL
are both derived in the methods section and the total loss is simply a
summation of the two. We plot the value of each loss as a function of the log
of the number of training iterations where one iteration is defined as a
training session over one batch of signals.\chris{Let's maybe start the plot at
1e3 iterations since the space seems wasted.}} 
\end{figure}

%
% Test procedure
%
After training has completed and we wish to use the network for inference we
follow the procedure described in the right hand panel of
Fig.~\ref{fig:network_config}. Given a new $y$ data sample (not taken from the
training set) we simply input this into the encoder $\textrm{E}_1$ from which
we obtain a single value of $\mu_{1}$ describing a distribution (conditional on
the data $y$) in the latent space. We then repeat the following 3 steps to
obtain new samples from the posterior

%~\chris{Hunter, note that we only need to
%do the first pass through E1 only once for a given peice of data do the real
%cost is only in the other steps. This will likely only change the timing by
%~25\% so it might not be worth doing the tests again. Just bear it in mind.}
%
\begin{itemize}
%
\item We randomly draw a latent space sample $z_1$. 
%
\item Our $z_1$ sample and the corresponding $y$ data are fed as input to our
pre-trained decoder network. The decoder network returns a set of moments
$\mu^{x}_1$ which describe a multivariate Gaussian distribution in the physical
parameter space.
%
\item We then draw a random $x$ realisation from that distribution. 
%
\end{itemize}
%
A comprehensive representation in the form of samples drawn from the entire
joint posterior distribution can then be obtained by simply repeating this
procedure with the same input data (see Eq.~\ref{eq:latent_model}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% results - here you would outline the process of comparison between the
% standard approach and the new one. Define training and test data and how Bilby
% is run on all test data for comparison. How do we then train our network. How
% do we then produce results on the test data. Here you refer to results plots
% but try to not make conclusion statememnts (just descriptive). Also include the
% speed analysis here.
%
%
% Intro to the results - what are we trying to do?
%
We present results on $256$ \ac{GW} test waveforms in simulated advanced detector noise and compare
between the existing Bayesian approaches and the \ac{CVAE}. Posteriors produced
by the \texttt{Bilby} inference library~\cite{1811.02042} are used as a
benchmark in order to assess the efficiency and quality of our machine learning
approach with the existing most optimal method for posterior sampling.

%
% describe the Bilby analysis 
%
For the benchmark analysis we assume that 5 parameters are unknown and use a
uniform prior on both black hole masses $m_1$ and $m_2$, which we allow to
range from $35$---$50$ solar masses, a uniform phase ($\phi_0$) prior spanning
$0$---$2\pi$ radians, a uniform distance ($d_{\text{L}})$ prior~\chris{we might
want to change this to be a $d$-squared prior to demonstrate the application of
priors in the \texttt{VItamin} training stage.} from $1$---$3$~Gpc, and a time of
coalescence ($t_{\text{c}}$) prior ranging from
$0.65$---$0.85$~seconds from the
start of the segment. The remainder of the parameters (sky position, binary
inclination angle, polarisation angle, and black hole spins) are all fixed at
constant values. We use timeseries durations of $1$~second with a sampling
frequency of $256$~Hz and the waveform model used is
\texttt{IMRPhenomPv2}~\cite{1809.10113} with a minimum cutoff frequency of
20Hz. For each input test waveform we run the benchmark analysis using multiple
sampling algorithms available within \texttt{Bilby} and for each algorithm we
run twice with different sampling random seeds. For each run and sampler we
extract $5000$ samples from the posterior on the 5 physical parameters.  

%
% the VItamin process
%
The \ac{CVAE} training process used as input $10^{6}$ waveforms corresponding
to parameters drawn from the same priors as assumed for the benchmark analysis.
The waveforms are also of identical duration, sampling frequency, and waveform
model and when each waveform is placed within a training batch it is given a
unique detector noise realisation. The data are then whitened using the same
advanced detector \ac{PSD} from which the simulated noise is generated. The
\ac{CVAE} posterior results are produced by passing our 256 whitened noisy
testing set of \ac{GW} waveforms as input into the testing path of the
pre-trained \ac{CVAE}. For each input waveform we sample until we have
generated $5000$ posterior samples on 4 physical parameters
($x=(m_1,m_2,d_{\text{L}},t_{0})$). We choose to output a subset of the full
5-dimensional space to demonstrate that nuisance parameters (such as $\phi_0$) 
can (if desired) be marginalized out within the \ac{CVAE} procedure
itself, rather than after training. 

%
% 1-D overlap results
%
\begin{figure*}
    \includegraphics[width=\textwidth]{images/corner_testcase0.png}
    \caption{\label{fig:corner_plot} Corner plot showing 2 and 1-dimensional
marginalised posterior distributions for one example test dataset. Filled (red)
contours represent the posteriors obtained form the \ac{CVAE} approach and
solid (blue) contours are the posteriors output from our benchmark analysis
(\texttt{Bilby} using the dynesty sampler). In each case the contour boundaries
enclose $68,90$ and $95\%$ probability. One dimensional histograms of the
posterior distribution for each parameter from both methods are plotted along
the diagonal. Blue and red vertical lines represent the $5$---$95\%$
non-symmetric confidence bounds for \texttt{Bilby} and variational inference
respectively. Black crosses and vertical lines denote the true parameter values
of the simulated signal.~\chris{Can you try to use the white space in the top
corner to show the original whitened timeseries for this test data?}} 
\end{figure*}

%
% discuss the corner plot results
%
We can imediately illustrate the accuracy of our machine learning predictions
by directly plotting posteriors (generated using the samples) generated by our
\ac{CVAE} and \texttt{Bilby} approaches superimposed on each other. We show
this for one example test dataset in Fig.~\ref{fig:corner_plot}, where the
strong agreement between both \texttt{Bilby} and the \ac{CVAE} can be clearly
seen. We provide additional test case example corner plots in the
methods section. \hunter{Add more corner plots in methods
section}~\chris{Maybe, but more corner plots aren't going to convince people.
We need to agregate the results into a single figure somehow (like the KL
values). I think something based on the 1-D intervals would be nice.}

%
% K-L divergence results
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/hist-kl.png}
    \caption{\label{fig:kl_results} We plot the KL-divergence values of
five independent samplers against each
other over the same 256 \ac{GW} test waveforms. Posterior samples are computed
for each of the 256 \ac{GW} waveforms using each sampler twice. A KL
divergence is then computed between all samplers with every other sampler
(including themselves) over all 256 \ac{GW} test cases. The distributions of the
resulting KL divergence values are then plotted, with each color representing a
different sampler combination. Both the $x$ and $y$ axes are scaled
logarithmically for readability.} 
\end{figure}
%

%
% discuss the KL results
%
Each independent sampler (including \ac{CVAE}) is run twice on the same test
data to produce samples from the corresponding posterior. We then compute the
KL-divergence between output distributions from each sampler with itself and
each sampler with all other samplers. For distributions that are identical the
KL-divergence is equal to zero but since we are representing our posterior
distributions using finite numbers of samples, identical distributions should
result in KL-divergence values $<1$. In Fig.~\ref{fig:kl_results} we show the
distributions of these KL-divergences for the 256 test \ac{GW} samples where
we see that samplers compared with themselves produce KL values that are
generally $<1$ with the exception of the Emcee sampler. More importantly we see
that the \ac{CVAE} approach when compared to the existing samplers have KL
values that match the values produced when comparing between 2 different
existing samplers.

%
% P-P plot
%
\begin{figure}
    \includegraphics[width=\columnwidth]{images/latest_pp_plot.png}
    \caption{\label{fig:pp_plot} P-P plot (defined in the main text) using 256
unique test samples and $5000$ posterior sample predictions per test sample.
The $x$-axis denotes the theoretical cumulative distribution whereas the y-axis
denotes the predicted cumulative distribution.~\chris{we might have to work on
the description of the axes - maybe use the same notation that has been used in
the GW PE papers that include p-p plots. Obviously this plot is quite worrying
but I can't see why you get these results looking at the test data posterior
comparison. Can you please check the p-p plot code - you should run it on the
Bilby samples as a reference anyway. To make the plot more interesting you
could also plot curves from each of the different bilby samplers (since you'll
have to run those anyway). Also, try to make the plot square - there is
supposed to be a symmetry between the axes.}}
\end{figure}
%

%
% discuss pp plot result
%
\chris{You need to better describe what a P-P plot is and why we use it. Maybe
throw in a reference to existing PE group papers that use it. Nature readers
will have never seen one before. We also have to wait until we get better P-P
plot curves for our approach before we warrant adding it to the paper.} In
order to show that our results are consistent with the truth, we have
additionally plotted probability-probability (p-p) plots in
Fig.~\ref{fig:pp_plot}. On the y axis is plotted the predicted cumulative
distribution and on the x axis is plotted the theoretical distribution.
Predictions from the \ac{CVAE} consistent with the black dashed diagonal line
is indicative that the Bayesian probability distributions are consistent with
the frequentest interpretation - that the truth will lie inside the $X\%$
probability contour with a frequency of $X\%$ of the time.  We plot empirical
results as those represented in the blue line. 

%
% discuss the speed of the analysis
%
The speed at which posterior samples are generated over all samplers used,
including \texttt{VItamin}, is shown in Table~\ref{Tab:speed}. Run-time is defined as
the total time to produce $\sim 5000$ samples for each sampler in seconds.
The major computational cost of running \texttt{VItamin} lies in the training time, which can 
take of order several hours to complete. However, once trained there is no need to 
retrain the network and may be run on any new signal which is consistent with 
the prior. It can be clearly seen that \texttt{VItamin} produces samples from the posterior at a rate which is $\sim 6 - 7$ orders of magnitude faster than current inference
techniques, representing a dramatic speed-up in performance. It should be noted
that run-time for the Bayesian samplers is also dependent on the choice of
sampler hyperparameters, so total run-time may vary according to hyperparameter
configuration. We choose Bayesian sampler hyperparameters based on standard default configurations defined in the \texttt{Bilby}
documentation~\cite{1811.02042}. 
 

%\chris{One more thing to add, especially if the other KL, AD, and PP plots
%aren't convincing, is a plot displaying 1D confidence bounds compared between
%bilby and VItamin. Imagine a plot with the x-axis as distance and the y-axis
%steps through test data with increasing true distance. for each test data you
%plot 2 error bars horizontally (one for bilby and one for VItamin) spanning the
%range of 90\% confidence. You would hopefully get nearly identical pairs of
%errorbars stacked vertically. Technically you could do this for all parameters
%(and you should) but we might only put one of the plots in the paper (if at
%all).}

%
% I feel the need, the need for speed, table
% 
\begin{table}
\centering
\caption{Durations required to produce $5000$ posterior samples from each of
the different posterior sampling approaches}
\begin{tabular}[t]{lcccc}
\toprule
& & run time (sec) & & \\
& min & max & median & Ratio \\
\hline
Dynesty & 602.3 & 1537.8 & 773.6 & $2.6\times 10^{-6}$ \\
Emcee & 2005.3 & 11926.5 & 4350.9 & $4.6\times 10^{-7}$ \\
Ptemcee & 3353.9 & 12771.4 & 4981.5 & $4.0\times 10^{-7}$ \\
Cpnest & 1430.7 & 5405.4 & 2286.9 & $8.8\times 10^{-7}$ \\
VItamin & - & - & $2.0\times 10^{-3}$ & 1.0 \\
\botrule
\end{tabular}
\label{Tab:speed}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% conclusions - now draw conclusions about the quality of the comparison
% results. Highlight the current limitations but also highlight the importance of
% this for the GW field (multi-detector is easy, additional parameters are easy,
% longer datasets may be a challenge regarding GPU memory?, we don't have to
% assume a noise model if we inject training data into real noise, we do rely on
% well defined signal models, EM-follow up in very low latency, can we use
% transfer learning if we want to retrain, ...) End with broader statements about
% inference in other fields and how this is applicable across the sciences.
%
% recap and main result
%
We currently face a variety of challenges in 
parameter estimation for the \ac{GW} field. Some of those 
challenges include accounting for glitches in the data, time-dependent 
fluctuations in the power spectrum density, systematic uncertainties 
in the waveform models themselves and decreasing the latency with 
which parameter posterior estimates are produced \cite{1409.7215}. 
In this letter we have demonstrated that we are able to
reproduce, to a high degree of accuracy, posteriors probability distributions generated from Bayesian
inference using machine learning. This is accomplished through the use of
variational autoencoders trained on simulated \ac{GW} signals. By building a neural network model which,
when trained, can reproduce the true posterior in less than a second,
we have demonstrated that neural networks can achieve the same 
quality of results as the trusted
benchmark, Bayesian inference.

%
% discuss the near term CBC implications and why this is a game-changer
%
The significance of achieving similar results
to Bayesian inference is most evident in the orders of magnitude increase in speed. This increase in speed will help the
LIGO-Virgo-KAGRA collaboration alert our \ac{EM} follow-up partners with
minimum latency. Improved low-latency alerts will be especially 
pertinent for \ac{BNS} signals like GW170817 where we will be able 
to catch prompt \ac{EM} emission on an even shorter time scale than 
is currently done with tools such as Bayestar \cite{1508.03634}. 
The estimated future detections for \ac{BNS} \hunter{ 
give number} will swamp our current computational resources using 
existing Bayesian methods. Our approach will provide (potentially) 
full-parameter estimation on \ac{BBH}/{\ac{BNS}} signals in seconds 
on a single GPU. Our trained pipeline is also incredibly modular, with 
one saved network and a small script to feed in new data, 
any user may produce results. Our analysis assumes a flat prior and our 
trains a machine learning box to essentially produce samples 
from the likelihood. Users may then have the unique ability to 
resample from the posterior according to their own prior choices. Our 
method may also be useful for population studies since populations 
may now be generated and analysed in a full-Bayesian manner on a smaller 
time scale. 

%
% future work, current limitations and prospects
%
We choose to use a sampling rate of 256Hz largely in order to 
decrease the time required to train our machine learning model. 
There is no general reason why our method 
should not work at larger sampling frequencies other than an 
increased amount of time to train and a larger 
number of GPUs required for memory consumption. The mass parameter space is limited 
to high mass signals, due to the loss of information on low mass 
signals at the low sampling frequency which we employ. Future work 
may also incorporate a multi-detector configuration by simply stacking 
1-dimensional time series vectors from multiple detector in one array 
per signal. We elected not to use a multi-detector configuration so 
that we may use relatively small datasets when training.

In reality, the detectors are also affected by non-Gaussian noise. 
To account for non-Gaussian noise, we would accordingly train our model using real noise from the detectors. Our method has the added 
advantage of not being dependent on choice of whitening, whereas 
current Bayesian methods are dependent on the whitening procedure. 
Our work can further be expanded upon by including a variety of
other \ac{GW} sources such as Neutron star - black hole and \ac{BNS} mergers at higher
sampling frequencies. We have yet to demonstrate the effectiveness of our method
on additional parameters such as sky location and inclination angle, the
benefits of which would be best realized in an end-to-end inference pipeline.
Such a pipeline would be of great importance as the detectors increase up to
their full potential design sensitivities. Given the abundant 
benefits of this method, we imagine a variant of 
this of pipeline will form the basis for all \ac{GW} parameter 
estimation over the next several decades to come.

%
% acknowledge people and funding agencies
%
\section{Acknowledgements.}
%
We would like to acknowledge valuable input from the LIGO-Virgo Collaboration
specifically from {\textbf{someone}}, and the parameter estimation and
machine-learning working groups. We thank Nvidia for the generous 
donation of a Tesla V-100 GPU. The authors also gratefully acknowledge the
Science and Technology Facilities Council of the United Kingdom. CM and SH are
supported by the Science and Technology Research Council (grant
No.~ST/~L000946/1) and the European Cooperation in Science and Technology
(COST) action CA17137.

%% Here is the endmatter stuff: Supplementary Info, etc.
%% Use \item's to separate, default label is "Acknowledgements"

\section{addendum}
 \subsection{Competing Interests} 
    The authors declare that they have no competing financial interests.
 \subsection{Correspondence} Correspondence and requests for materials should be addressed to Hunter Gabbard~(email: h.gabbard.1@research.gla.ac.uk).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% methods - Everything that we couldn't fit in. Mostly validation plots.
%
\section{Methods}
%
Put methods in here.  If you are going to subsection it, use
\verb|\subsection| commands.  Methods section should be less than
800 words and if it is less than 200 words, it can be incorporated
into the main text.

\subsection{Whitening} \label{whiten_sec}

In order to ensure 
that there is equal power accross all
frequency bins of our signal, we ensure that the noise is whitened Gaussian
noise. We whiten using a single detector (H1) power spectrum density derived from the
Advanced LIGO design sensitivity curves \cite{2016LRR....19....1A}. 

\subsection{Nested Sampling} \label{nested_sec}
%
% Describe nested sampling algorithm
%
If we would like to find the results for a particular parameter, 
we simply marginalize over all unwanted parameters 

\begin{equation}
    p(x_1|y) = \int dx_2 ... dx_N p(x|y).
\end{equation}

There are several algorithms which may be used to sample from the posterior
distribution of astrophysical GW source parameters \cite{PhysRevD.64.022001,
skilling2006,10.1111/j.1365-2966.2011.20288.x}. The
algorithm which is used in our case is the nested sampling algorithm. Nested
sampling takes a multi-dimensional evidence integral calculation (fully
marginalized likelihood) and transforms it into a more manageable 1-D integral
, where evidence is the integral of the likelihood 
multiplied by the prior over all parameters of the model $h$ ~\cite{1409.7215}

\begin{equation}
    Z = p(y|h) = \int dx_1 ... dx_n p(y|x,h)p(x|h).\label{eq:evidence}
\end{equation}

%
% More nested sampling description
%
The first step of the nested sampling algorithm starts by generating an initial
set of live points made from the prior distribution. The likelihood for each
point is calculated and the point with the lowest likelihood is removed. The
removed sample is then replaced with a new sample which has a higher
likelihood. This cycle repeats itself until a predefined stopping threshold is
achieved \cite{1409.7215} . Samples from the posterior may be drawn by randomly
selecting from both all current 'live' points and all previously removed 'live'
points.

The nested sampling algorithm is run using
1000 live points and has a predefined stopping criteria of $0.1$~\chris{expand
upon what the stopping criteria means. This is related to the evidence
calculation and the tolerance on that result}. The sampler takes $\mathcal{O}(3
\: \textrm{minutes})$ to converge~\chris{jargon. What does converge mean to the
average Nature reader?}. After the nested sampler has converged~\chris{jargon}, we draw
samples~\chris{too technical with not enough explanation. The \texttt{Bilby} analysis
produces posterior parameter estimates and does so by supplying the user with
samples drawn from that distribution.} and produce a posterior on the
astrophysical \ac{GW} source parameters we are trying to estimate.

\subsection{Loss function derivation} \label{lossDer_sec}

This section is to be rephrased into appropriate language but for now I will
add my interpretation of the loss function and the design of the network.

We begin with the statement defining the aim of the analysis. We wish to obtain
a function that reproduces the posterior distribution (the probability of our
physical parameters given some measured data). We define the cross entropy
between 2 distributions as
%
\begin{equation}\label{eq:cross_ent}
H(p,r) = -\int dx\, p(x|y) \log r_{\theta}(x|y)
\end{equation}
%
where we have made the distributions explicitly conditional on $y$ (our
measurement). In this case $p(x|y)$ is the target distribution (the true
posterior) and $r_{\theta}(x|y)$ is the parametric distribution that we will
use neural networks to construct. In this case $\theta$ represents the
trainable neural network parameters. 

The cross-entropy is minimised when $p(x|y)=r_{\theta}(x|y)$ and so by maximising
%
\begin{equation}\label{eq:cost1}
C = \text{E}_{p(y)}\left[\int dx\,p(x|y) \log r_{\theta}(x|y)\right]
\end{equation}
% 
where $\text{E}_{p(y)}[\dot]$ indicates the expectation value over the
distribution of measurements, we therefore make the parameteric distribution as
similar to the target for all possible measurements $y$.

Converting the expectation value into an integral over $y$ weighted by $p(y)$
and applying Bayes theorem we obtain
%
\begin{equation}\label{eq:cost1}
C = \int dx\,p(x)\int dy\,p(y|x)\log r_{\theta}(x|y)
\end{equation}
%
where $p(x)$ is the prior distribution on the physical parameters $x$.

The conditional variational autoencoder network outlined in
Fig.~\ref{fig:network_config} makes use of a conditional latent variable model.
Our parameteric model is constructed from the product of 2 seperate
distributions marginalised over the latent space
%
\begin{equation}\label{eq:latent_model}
r_{\theta}(x|y) = \int dz\,r_{\theta_{1}}(z|y)r_{\theta_{2}}(x|z,y).
\end{equation}
%  
We have used $\theta_{1}$ and $\theta_{2}$ to indidate that the 2 seperate
networks modelling these distributions will be trained on these parameter sets
respectively. Both new conditional distributions are modelled as $n_{z}$
dimensional multivariate uncorrelated Gaussian distributions (governed by their
means and variances). However, this still allows $r_{\theta}(x|y)$ to take a
general form (although it does limit it to be unimodal).  

One could be forgiven in thinking that by setting up networks that simply aim
to maximise $C$ over the $\theta_{1}$ and $\theta_{2}$ would be enough to solve
this problem. However, as shown in~\cite{NIPS2015_5775} this is an intractable
problem and a network cannot be trained directly to do this. Instead we define
a recognition function $q_{\phi}(z|x,y)$ that will be used to derive an
\ac{ELBO}.

Let us first define the Kullback–Leibler divergence between 2 of our
distributions as
%
\begin{equation}\label{eq:kl}
\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{2}}(z|x,y)\right] = \int dz\,
\log\left(\frac{q_{\phi}(z|x,y)}{r_{\theta_{2}}(z|x,y)}\right).
\end{equation}
%  
It can be shown that after some manipulation that
%
\begin{equation}\label{eq:elbo1}
\log r_{\theta}(x|y) = L + \text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{?}}(z|x,y)\right]
\end{equation}
%
where the \ac{ELBO} $L$ is given by
%
\begin{equation}\label{eq:elbo2}
L = \int dz\,
q_{\phi}(z|x,y)\log\left(\frac{r_{\theta_{2}}(x|z,y)r_{\theta_{1}}(z|y)}{q_{\phi}(z|x,y)}\right)
\end{equation}
%
and is so-named since $\text{KL}$ cannot be negative and has a minimum of zero.
Therefore, if we were to find a $q_{\phi}(z|x,y)$ function (optimised on
$\phi$) that minimised the KL-divergence then we can state that
%
\begin{equation}
\log r_{\theta}(x|y) \geq L.
\end{equation}
%
After some further manipulation of Eq.\ref{eq:elbo2} we find that
%
\begin{align}\label{eq:logr}
\log r_{\theta}(x|y) \geq & \text{E}_{q_{\phi}(z|x,y)}\left[\log
r_{\theta_{2}}(x|z,y)\right] \nonumber\\
&-\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{1}}(z|y)\right].
\end{align}
%
We can now substitute this inequality into Eq.~\ref{eq:cost1} (our cost
function) to obtain
%
\begin{align}\label{eq:cost2}
C \geq & \int dx\, p(x)\int dy\,p(y|x)
\left[\text{E}_{q_{\phi}(z|x,y)}\left[\log r_{\theta_{2}}(x|z,y)\right]
\right.\nonumber\\
&-\left.\text{KL}\left[q_{\phi}(z|x,y)||r_{\theta_{1}}(z|y)\right]\right]  
\end{align}
%
which can in practice be approximated as a stochastic integral over draws of
$x$ from the prior, $y$ from the likelihood function $p(y|x)$, and from the
recognition function, giving us
%
\begin{align}\label{eq:cost3}
C \geq & \frac{1}{N}\sum_{n=1}^{N}\left[\log
r_{\theta_{2}}(x_{n}|z_{n},y_{n})\right.\nonumber\\
&\left.-\text{KL}\left[q_{\phi}(z|x_{n},y_{n})||r_{\theta_{1}}(z|y_{n})\right]\right].
\end{align}
% 
In this case we draw $N$ sets of physical parameter, corresponding data
sets, and draws from the recognition function. It is important to note that
whilst it is true that the KL-divergence between the $q_{\phi}(z|x,y)$ and
$r_{\theta_{2}}(z|x,y)$ (seen in Eq.~\ref{eq:elbo1}) should be minimised and in
an ideal case be equal to zero, that is not the case for the KL-divergence in
Eq.~\ref{eq:cost3}. In this case our aim is to maximise $C$ which implies that
the KL-divergence should be low but not necessarily zero.

We have now set up a system composed of 3 functions that have well defined
inputs and outputs where the mapping of those inputs to outputs is governed by
the parameter sets $\theta_{1},\theta_{2},\phi$. These parameters are the
weights and biases of 3 neural networks acting as (variational) encoder,
decoder, and encoder respectively. To train such a network one must connect the
inputs and outputs appropriately to compute $C$ and back-propogate cost
function derivatives to update the network parameters. The network structure
shown in Fig.~\ref{fig:network_config} shows how for a batch of $N$ sets of $x$
and corresponding $y$ values first both $x$ and $y$ are fed into the encoder 2
network that produces the means and variances of the latent space $z$
describing the $q_{\phi}(z|x,y)$ distribution function. Next (the order does
not matter) the data $y$ alone is passed into the encoder 1 network that
produces the means and variances of the latent space $z$ describing the
$r_{\theta_{1}}(z|y)$ distribution function. Given the parameters of these
Gaussian (diagonal covariance matrix) distributions the $\text{KL}$-divergence
is computed as
%
\begin{align}\label{eq:klgauss}
\text{KL}\left[q_{\phi}(z|x_{n},y_{n})||r_{\theta_{1}}(z|y_{n})\right] =&
\frac{1}{2}\sum_{j=1}^{n_{z}}\left[\frac{\sigma_{q,j}^{2}}{\sigma_{r,j}^{2}} +
\frac{(\mu_{r,j}-\mu_{q,j})^{2}}{\sigma_{r,j}^{2}}\right.\nonumber\\ 
&\left.+
\log\left(\frac{\sigma_{r,j}^{2}}{\sigma_{q,j}^{2}}\right)\right] -
\frac{n_{z}}{2}
\end{align}
%
where those labelled and indexed parameters output from the encoder networks
are denoted by $\mu$ and $\sigma$.

To evaluate the first term within the loss function (Eq.~\ref{eq:cost3}) we
require the data $y$ and a randomly drawn $z$ value from the latent space defined
by $q_{\phi}(z|x,y)$. These are then passed as input into the decoder network D
that produces the means and variances in the physical parameter space $x$ describing
the $r_{\theta_{2}}(x|z,y)$ distribution function. With the function now well
defined we simply evaluate the function at the values of the input $x_{n}$
values. Training of the network proceeds until a convergence of the cost is
seen. 

When the network has been trained it can be used for the inference of unknown
physical parameters given a new single instance of measured data $y$. To do
this one is interested in obtaining samples from the distribution
$r_{\theta}(x|y)$ which can be obtained by evaluating
Eq.~\ref{eq:latent_model}. To do this using the network we first obtain the
parameters governing the function $r_{\theta_{1}}(z|y)$ by passing the data $y$
into the encoder 1 network. Using the output means and variances on the latent space
$z$ we then draw from that space to obtain a random latent space realisation.
This $z$ value and the original $y$ data is then input into the decoder network
which outputs the mean and variance parameters governing the function
$r_{\theta_{2}}(x|z,y)$. The next step is to draw a random $x$ realisation from
that function. This is representative of a posterior sample from the target
distribution $p(x|y)$ and to obtain an informative description of the entire
distribution one would repeat the procedure with the same input data
$\mathcal{O}(1000)$ times from which parameter confidence bounds can be
determined.      


\bibliographystyle{apsrev4-1}
\bibliography{references}% Produces the bibliography via BibTeX.

\end{document}
