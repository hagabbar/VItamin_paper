\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{PhysRevX.6.041015,PhysRevLett.119.161101}
\citation{2018LRR....21....3A}
\citation{2009CQGra..26o5017S}
\citation{1409.7215}
\citation{gracedb_O3}
\citation{2016PhRvD..93b4013S}
\citation{1904.06264,1812.04405}
\citation{GEORGE201864,PhysRevLett.120.141103,GebKilParHarSch}
\citation{2009CQGra..26o5017S}
\citation{skilling2006,cpnest,dynesty}
\citation{emcee,ptemcee}
\citation{1409.7215,1811.02042}
\citation{GEORGE201864,PhysRevLett.120.141103,GebKilParHarSch}
\citation{0264-9381-34-6-064003}
\citation{Coughlin_2017}
\citation{2012MNRAS.421..169G}
\citation{2019arXiv190905966C}
\citation{2020arXiv200207656G}
\citation{Cranmer201912789}
\newlabel{FirstPage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {title}{Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy}{1}{section*.1}\protected@file@percent }
\newacro{GW}[GW]{Gravitational wave}
\newacro{BBH}[BBH]{binary black hole}
\newacro{EM}[EM]{electromagnetic}
\newacro{CBC}[CBC]{compact binary coalescence}
\newacro{BNS}[BNS]{binary neutron star}
\newacro{NSBH}[NSBH]{neutron star black hole}
\newacro{PSD}[PSD]{power spectral density}
\newacro{ELBO}[ELBO]{evidence lower bound}
\newacro{LIGO}[LIGO]{advanced Laser Interferometer Gravitational wave Observatory}
\newacro{CVAE}[CVAE]{conditional variational autoencoder}
\newacro{KL}[KL]{Kullback--Leibler}
\newacro{GPU}[GPU]{graphics processing unit}
\newacro{LVC}[LVC]{LIGO-Virgo Collaboration}
\newacro{PP}[p-p]{probability-probability}
\newacro{SNR}[SNR]{signal-to-noise ratio}
\AC@undonewlabel{acro:GW}
\newlabel{acro:GW}{{}{1}{}{section*.2}{}}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\AC@undonewlabel{acro:BNS}
\newlabel{acro:BNS}{{}{1}{}{section*.3}{}}
\acronymused{BNS}
\AC@undonewlabel{acro:NSBH}
\newlabel{acro:NSBH}{{}{1}{}{section*.4}{}}
\acronymused{NSBH}
\AC@undonewlabel{acro:EM}
\newlabel{acro:EM}{{}{1}{}{section*.5}{}}
\acronymused{EM}
\acronymused{EM}
\AC@undonewlabel{acro:CVAE}
\newlabel{acro:CVAE}{{}{1}{}{section*.6}{}}
\acronymused{CVAE}
\AC@undonewlabel{acro:BBH}
\newlabel{acro:BBH}{{}{1}{}{section*.7}{}}
\acronymused{BBH}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\newlabel{eq:bayes_theorem}{{1}{1}{}{equation.0.1}{}}
\acronymused{GW}
\AC@undonewlabel{acro:LIGO}
\newlabel{acro:LIGO}{{}{1}{}{section*.8}{}}
\acronymused{LIGO}
\acronymused{GW}
\acronymused{GW}
\acronymused{CVAE}
\acronymused{GW}
\citation{1904.06264,NIPS2015_5775}
\citation{1512.00570}
\citation{1612.00005}
\citation{1807.03653}
\citation{1904.06264}
\citation{aligo_noisecurves}
\citation{1811.02042}
\citation{1809.10113}
\acronymused{CVAE}
\acronymused{CVAE}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{CVAE}
\newlabel{eq:cross_ent}{{2}{2}{}{equation.0.2}{}}
\newlabel{eq:latent_model}{{3}{2}{}{equation.0.3}{}}
\AC@undonewlabel{acro:ELBO}
\newlabel{acro:ELBO}{{}{2}{}{section*.9}{}}
\acronymused{ELBO}
\newlabel{eq:cost3}{{4}{2}{}{equation.0.4}{}}
\AC@undonewlabel{acro:KL}
\newlabel{acro:KL}{{}{2}{}{section*.10}{}}
\acronymused{KL}
\acronymused{CVAE}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{BBH}
\acronymused{CVAE}
\acronymused{GW}
\AC@undonewlabel{acro:SNR}
\newlabel{acro:SNR}{{}{2}{}{section*.11}{}}
\acronymused{SNR}
\acronymused{BBH}
\citation{aligo_noisecurves}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The configuration of the \ac {CVAE} neural network. During training (left-hand side), a training set of noisy \ac {GW} signals ($y$) and their corresponding true parameters ($x$) are given as input to encoder network \leavevmode {\color  {red}$q_{\phi }$}, while only $y$ is given to encoder network \leavevmode {\color  {red}$r_{\theta _1}$}. The \ac {KL}-divergence (Eq.\nobreakspace  {}\ref  {eq:kl}) is computed between the encoder output latent space representations ($\mu _q$ and $\mu _r$) forming one component of the total cost function. Samples ($z_q$) from the \leavevmode {\color  {red}$q_{\phi }$} latent space representation are generated and passed to the decoder network \leavevmode {\color  {red}$r_{\theta _2}$} together with the original input data $y$. The output of the decoder ($\mu _x$) describes a distribution in the physical parameter space and the cost component $L$ is computed by evaluating that distribution at the \leavevmode {\color  {red}location} of the original input $x$. When performed in batches this scheme allows the computation of the total cost function Eq.\nobreakspace  {}\ref  {eq:cost3}. After having trained the network \leavevmode {\color  {red}and therefore having minimised the cross-entropy $H$}, we test (right-hand side) using only the \leavevmode {\color  {red}$r_{\theta _1}$} encoder and the \leavevmode {\color  {red}$r_{\theta _2}$} decoder to produce samples ($x_{\text  {samp}}$). \leavevmode {\color  {red}These samples are drawn from the distribution $r_{\theta }(x|y)$ (Eq.\nobreakspace  {}\ref  {eq:latent_model}) and accurately model the true posterior $p(x|y)$.}}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:network_config}{{1}{3}{The configuration of the \ac {CVAE} neural network. During training (left-hand side), a training set of noisy \ac {GW} signals ($y$) and their corresponding true parameters ($x$) are given as input to encoder network \new {$q_{\phi }$}, while only $y$ is given to encoder network \new {$r_{\theta _1}$}. The \ac {KL}-divergence (Eq.~\ref {eq:kl}) is computed between the encoder output latent space representations ($\mu _q$ and $\mu _r$) forming one component of the total cost function. Samples ($z_q$) from the \new {$q_{\phi }$} latent space representation are generated and passed to the decoder network \new {$r_{\theta _2}$} together with the original input data $y$. The output of the decoder ($\mu _x$) describes a distribution in the physical parameter space and the cost component $L$ is computed by evaluating that distribution at the \new {location} of the original input $x$. When performed in batches this scheme allows the computation of the total cost function Eq.~\ref {eq:cost3}. After having trained the network \new {and therefore having minimised the cross-entropy $H$}, we test (right-hand side) using only the \new {$r_{\theta _1}$} encoder and the \new {$r_{\theta _2}$} decoder to produce samples ($x_{\text {samp}}$). \new {These samples are drawn from the distribution $r_{\theta }(x|y)$ (Eq.~\ref {eq:latent_model}) and accurately model the true posterior $p(x|y)$.}}{figure.1}{}}
\acronymused{CVAE}
\acronymused{GW}
\acronymused{KL}
\acronymused{PSD}
\acronymused{GW}
\acronymused{CVAE}
\acronymused{CVAE}
\acronymused{CVAE}
\acronymused{CVAE}
\AC@undonewlabel{acro:PP}
\newlabel{acro:PP}{{}{3}{}{section*.13}{}}
\acronymused{PP}
\acronymused{KL}
\acronymused{PP}
\AC@undonewlabel{acro:PSD}
\newlabel{acro:PSD}{{2}{3}{}{section*.12}{}}
\acronymused{PSD}
\acronymused{PSD}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Corner plot showing one and two-dimensional marginalised posterior distributions \leavevmode {\color  {red}on the \ac {GW} parameters} for one example test dataset. Filled \leavevmode {\color  {red}red} contours represent the \leavevmode {\color  {red}two-dimensional joint} posteriors obtained from \leavevmode {\color  {red}\texttt  {VItamin}} and solid \leavevmode {\color  {red}blue and green} contours are the \leavevmode {\color  {red}corresponding} posteriors output from our \leavevmode {\color  {red}benchmark} analyses (\leavevmode {\color  {red}using the \texttt  {Dynesty} and \texttt  {ptemcee} samplers within \texttt  {Bilby}}). In each case, the contour boundaries enclose $68,90$ and $95\%$ probability. One dimensional histograms of the posterior distribution for each parameter from both methods are plotted along the diagonal. \leavevmode {\color  {red}Black vertical and horizontal lines denote the true parameter values of the simulated signal. At the top of the figure we include a Mollweide projection of the sky location posteriors from all three analyses. All results presented in this letter correspond to a three-detector configuration but for clarity we only plot the H1 whitened noisy time-series $y$ and the noise-free whitened signal (in blue and cyan respectively) to the right of the figure. The test signal was simulated with an optimal multi-detector signal-to-noise ratio of 17.2.}}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:corner_plot}{{2}{4}{Corner plot showing one and two-dimensional marginalised posterior distributions \new {on the \ac {GW} parameters} for one example test dataset. Filled \new {red} contours represent the \new {two-dimensional joint} posteriors obtained from \new {\texttt {VItamin}} and solid \new {blue and green} contours are the \new {corresponding} posteriors output from our \new {benchmark} analyses (\new {using the \texttt {Dynesty} and \texttt {ptemcee} samplers within \texttt {Bilby}}). In each case, the contour boundaries enclose $68,90$ and $95\%$ probability. One dimensional histograms of the posterior distribution for each parameter from both methods are plotted along the diagonal. \new {Black vertical and horizontal lines denote the true parameter values of the simulated signal. At the top of the figure we include a Mollweide projection of the sky location posteriors from all three analyses. All results presented in this letter correspond to a three-detector configuration but for clarity we only plot the H1 whitened noisy time-series $y$ and the noise-free whitened signal (in blue and cyan respectively) to the right of the figure. The test signal was simulated with an optimal multi-detector signal-to-noise ratio of 17.2.}}{figure.2}{}}
\acronymused{GW}
\citation{dynesty}
\citation{2016PhRvD..94d4031S,2019PhRvD..99h4026W,2019PhRvD.100d3030T}
\citation{emcee}
\citation{ptemcee}
\citation{cpnest}
\citation{PhysRevLett.119.161101}
\citation{2020ApJ...892L...3A}
\citation{2016PhRvD..93b4013S}
\citation{2018LRR....21....3A}
\citation{2015PhRvD..91h4034L}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Durations required to produce samples from each of the different posterior sampling approaches.}}{5}{table.1}\protected@file@percent }
\newlabel{Tab:speed}{{}{5}{}{table.1}{}}
\acronymused{KL}
\acronymused{BBH}
\acronymused{CVAE}
\acronymused{GW}
\acronymused{BBH}
\acronymused{BNS}
\acronymused{NSBH}
\acronymused{EM}
\AC@undonewlabel{acro:LVC}
\newlabel{acro:LVC}{{}{5}{}{section*.14}{}}
\acronymused{LVC}
\acronymused{BNS}
\acronymused{GW}
\AC@undonewlabel{acro:CBC}
\newlabel{acro:CBC}{{}{5}{}{section*.15}{}}
\acronymused{CBC}
\AC@undonewlabel{acro:GPU}
\newlabel{acro:GPU}{{}{5}{}{section*.16}{}}
\acronymused{GPU}
\acronymused{BBH}
\acronymused{GW}
\acronymused{GPU}
\acronymused{GW}
\acronymused{PSD}
\acronymused{GW}
\acronymused{EM}
\citation{gallinari1987memoires}
\citation{1812.04405}
\acronymused{PSD}
\acronymused{CVAE}
\acronymused{PSD}
\acronymused{PSD}
\acronymused{CBC}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\@writefile{toc}{\contentsline {section}{\numberline {}Acknowledgements.}{6}{section*.17}\protected@file@percent }
\acronymused{LVC}
\@writefile{toc}{\contentsline {section}{\numberline {}addendum}{6}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {}Competing Interests}{6}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {}Correspondence}{6}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {}Methods}{6}{section*.21}\protected@file@percent }
\newlabel{sec:methods}{{}{6}{}{section*.21}{}}
\acronymused{CVAE}
\acronymused{GW}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Cost function derivation}{6}{section*.22}\protected@file@percent }
\citation{NIPS2015_5775}
\newlabel{eq:cost1}{{5}{7}{}{equation.0.5}{}}
\newlabel{eq:cost1}{{6}{7}{}{equation.0.6}{}}
\acronymused{CVAE}
\acronymused{ELBO}
\acronymused{KL}
\newlabel{eq:kl}{{7}{7}{}{equation.0.7}{}}
\newlabel{eq:elbo1}{{8}{7}{}{equation.0.8}{}}
\acronymused{ELBO}
\newlabel{eq:elbo2}{{9}{7}{}{equation.0.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The uniform prior boundaries and fixed parameter values used on the \ac {BBH} signal parameters for the benchmark and the \ac {CVAE} analyses.}}{7}{table.2}\protected@file@percent }
\acronymused{BBH}
\acronymused{CVAE}
\newlabel{tab:prior_ranges}{{}{7}{}{table.2}{}}
\acronymused{KL}
\acronymused{KL}
\newlabel{eq:logr}{{11}{7}{}{equation.0.11}{}}
\newlabel{eq:cost2}{{12}{7}{}{equation.0.12}{}}
\acronymused{CVAE}
\acronymused{CVAE}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Network design}{7}{section*.23}\protected@file@percent }
\newlabel{sec:network_design}{{}{7}{}{section*.23}{}}
\acronymused{CVAE}
\citation{Siria2020.06.11.144253}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The cost as a function of training iteration. \leavevmode {\color  {red}We show the total cost function (green) together with its component parts: the \ac {KL}-divergence component (orange) and the reconstruction component (blue) which are simply summed to obtain the total. The solid curves correspond to the cost computed on each batch of training data and the dashed curves represent the cost when computed on independent validation data. The close agreement between training and validation cost values indicates that the network is not overfitting to the training data. The change in behavior of the cost between $10^4$ and $10^5$ iterations is a consequence of gradually introducing the \ac {KL} cost term contribution via an annealing process.}}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:loss_log}{{3}{8}{The cost as a function of training iteration. \new {We show the total cost function (green) together with its component parts: the \ac {KL}-divergence component (orange) and the reconstruction component (blue) which are simply summed to obtain the total. The solid curves correspond to the cost computed on each batch of training data and the dashed curves represent the cost when computed on independent validation data. The close agreement between training and validation cost values indicates that the network is not overfitting to the training data. The change in behavior of the cost between $10^4$ and $10^5$ iterations is a consequence of gradually introducing the \ac {KL} cost term contribution via an annealing process.}}{figure.3}{}}
\acronymused{KL}
\acronymused{KL}
\acronymused{GW}
\acronymused{GW}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces The \texttt  {VItamin} network hyper-parameters}}{9}{table.3}\protected@file@percent }
\newlabel{Tab:network_design}{{}{9}{}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Training procedure}{9}{section*.24}\protected@file@percent }
\newlabel{app:training_procedure}{{}{9}{}{section*.24}{}}
\acronymused{GW}
\acronymused{GW}
\acronymused{KL}
\newlabel{eq:klgauss}{{13}{10}{}{equation.0.13}{}}
\acronymused{KL}
\acronymused{KL}
\acronymused{KL}
\acronymused{KL}
\acronymused{BBH}
\acronymused{PP}
\acronymused{KL}
\acronymused{GPU}
\acronymused{GPU}
\@writefile{toc}{\contentsline {subsection}{\numberline {}The testing procedure}{10}{section*.25}\protected@file@percent }
\citation{1811.02042}
\citation{1811.02042}
\citation{dynesty}
\citation{ptemcee}
\citation{cpnest}
\citation{emcee}
\citation{1409.7215}
\citation{4839047}
\bibdata{VItamin_red_textNotes,references}
\bibcite{PhysRevX.6.041015}{{1}{2016}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{PhysRevLett.119.161101}{{2}{2017}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{2018LRR....21....3A}{{3}{2018}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{2009CQGra..26o5017S}{{4}{2009}{{{Searle}\ \emph  {et~al.}}}{{{Searle}, {Sutton},\ and\ {Tinto}}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  One-dimensional \ac {PP} plots for each parameter and \leavevmode {\color  {red}for} each benchmark sampler and \texttt  {VItamin}. The curves were constructed using the 256 test datasets \leavevmode {\color  {red}and the} dashed black diagonal line indicates the ideal result. The best and worst-case $p$-values associated with each sampling method are (0.972,0.211 \texttt  {VItamin}), (0.832,0.043 \texttt  {Dynesty}), (0.728,0.117 \texttt  {ptemcee}), (0.840,0.189 \texttt  {CPNest}), (0.489,0.002 \texttt  {emcee}). }}{11}{figure.4}\protected@file@percent }
\newlabel{fig:pp_plot}{{4}{11}{One-dimensional \ac {PP} plots for each parameter and \new {for} each benchmark sampler and \texttt {VItamin}. The curves were constructed using the 256 test datasets \new {and the} dashed black diagonal line indicates the ideal result. The best and worst-case $p$-values associated with each sampling method are (0.972,0.211 \texttt {VItamin}), (0.832,0.043 \texttt {Dynesty}), (0.728,0.117 \texttt {ptemcee}), (0.840,0.189 \texttt {CPNest}), (0.489,0.002 \texttt {emcee})}{figure.4}{}}
\acronymused{PP}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Benchmark sampler configuration parameters. Values were chosen based on a combination of their recommended default parameters\nobreakspace  {}\cite  {1811.02042} and private communication with the \texttt  {Bilby} development team. }}{11}{table.4}\protected@file@percent }
\newlabel{Tab:sampler_params}{{}{11}{}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Additional tests}{11}{section*.26}\protected@file@percent }
\acronymused{GW}
\acronymused{PP}
\acronymused{KL}
\acronymused{CVAE}
\acronymused{KL}
\acronymused{KL}
\acronymused{KL}
\acronymused{KL}
\acronymused{GW}
\acronymused{KL}
\acronymused{KL}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{11}{section*.27}\protected@file@percent }
\bibcite{1409.7215}{{5}{2014}{{Veitch\ \emph  {et~al.}}}{{Veitch, Raymond, Farr, Farr, Graff, Vitale, Aylott, Blackburn, Christensen, Coughlin, Pozzo, Feroz, Gair, Haster, Kalogera, Littenberg, Mandel, O'Shaughnessy, Pitkin, Rodriguez, R\IeC {\"o}ver, Sidery, Smith, Sluys, Vecchio, Vousden,\ and\ Wade}}}
\bibcite{gracedb_O3}{{6}{}{{gra}}{{}}}
\bibcite{2016PhRvD..93b4013S}{{7}{2016}{{{Singer}\ and\ {Price}}}{{}}}
\bibcite{1904.06264}{{8}{2019}{{Tonolini\ \emph  {et~al.}}}{{Tonolini, Lyons, Caramazza, Faccio,\ and\ Murray-Smith}}}
\bibcite{1812.04405}{{9}{2018}{{Pagnoni\ \emph  {et~al.}}}{{Pagnoni, Liu,\ and\ Li}}}
\bibcite{GEORGE201864}{{10}{2018}{{George\ and\ Huerta}}{{}}}
\bibcite{PhysRevLett.120.141103}{{11}{2018}{{Gabbard\ \emph  {et~al.}}}{{Gabbard, Williams, Hayes,\ and\ Messenger}}}
\bibcite{GebKilParHarSch}{{12}{2017}{{Gebhard\ \emph  {et~al.}}}{{Gebhard, Kilbertus, Parascandolo, Harry,\ and\ Sch{\"o}lkopf}}}
\bibcite{skilling2006}{{13}{2006}{{Skilling}}{{}}}
\bibcite{cpnest}{{14}{2017}{{Veitch\ \emph  {et~al.}}}{{Veitch, Pozzo, Messick,\ and\ Pitkin}}}
\bibcite{dynesty}{{15}{2019}{{Speagle}}{{}}}
\bibcite{emcee}{{16}{2013}{{{Foreman-Mackey}\ \emph  {et~al.}}}{{{Foreman-Mackey}, {Hogg}, {Lang},\ and\ {Goodman}}}}
\bibcite{ptemcee}{{17}{2015}{{Vousden\ \emph  {et~al.}}}{{Vousden, Farr,\ and\ Mandel}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Distributions of \ac {KL}-divergence values between posteriors produced by different samplers. \leavevmode {\color  {red}In each panel we show the distribution of \ac {KL}-divergences computed between a single benchmark sampler and every other benchmark sampler over all 256 \ac {GW} test cases (grey). Also plotted in each panel are the corresponding \ac {KL}-divergence distributions between the single benchmark sampler and the \texttt  {VItamin} outputs (blue, green, purple, yellow).}}}{12}{figure.5}\protected@file@percent }
\newlabel{fig:kl_results}{{5}{12}{Distributions of \ac {KL}-divergence values between posteriors produced by different samplers. \new {In each panel we show the distribution of \ac {KL}-divergences computed between a single benchmark sampler and every other benchmark sampler over all 256 \ac {GW} test cases (grey). Also plotted in each panel are the corresponding \ac {KL}-divergence distributions between the single benchmark sampler and the \texttt {VItamin} outputs (blue, green, purple, yellow).}}{figure.5}{}}
\acronymused{KL}
\acronymused{KL}
\acronymused{GW}
\acronymused{KL}
\bibcite{1811.02042}{{18}{2018}{{Ashton\ \emph  {et~al.}}}{{Ashton, Huebner, Lasky, Talbot, Ackley, Biscoveanu, Chu, Divarkala, Easter, Goncharov, Vivanco, Harms, Lower, Meadors, Melchor, Payne, Pitkin, Powell, Sarin, Smith,\ and\ Thrane}}}
\bibcite{0264-9381-34-6-064003}{{19}{2017}{{Zevin\ \emph  {et~al.}}}{{Zevin, Coughlin, Bahaadini, Besler, Rohani, Allen, Cabero, Crowston, Katsaggelos, Larson, Lee, Lintott, Littenberg, Lundgren, \IeC {\O }sterlund, Smith, Trouille,\ and\ Kalogera}}}
\bibcite{Coughlin_2017}{{20}{2017}{{Coughlin\ \emph  {et~al.}}}{{Coughlin, Earle, Harms, Biscans, Buchanan, Coughlin, Donovan, Fee, Gabbard, Guy, Mukund,\ and\ Perry}}}
\bibcite{2012MNRAS.421..169G}{{21}{2012}{{{Graff}\ \emph  {et~al.}}}{{{Graff}, {Feroz}, {Hobson},\ and\ {Lasenby}}}}
\bibcite{2019arXiv190905966C}{{22}{2019}{{{Chua}\ and\ {Vallisneri}}}{{}}}
\bibcite{2020arXiv200207656G}{{23}{2020}{{{Green}\ \emph  {et~al.}}}{{{Green}, {Simpson},\ and\ {Gair}}}}
\bibcite{Cranmer201912789}{{24}{2020}{{Cranmer\ \emph  {et~al.}}}{{Cranmer, Brehmer,\ and\ Louppe}}}
\bibcite{NIPS2015_5775}{{25}{2015}{{Sohn\ \emph  {et~al.}}}{{Sohn, Lee,\ and\ Yan}}}
\bibcite{1512.00570}{{26}{2015}{{Yan\ \emph  {et~al.}}}{{Yan, Yang, Sohn,\ and\ Lee}}}
\bibcite{1612.00005}{{27}{2016}{{Nguyen\ \emph  {et~al.}}}{{Nguyen, Clune, Bengio, Dosovitskiy,\ and\ Yosinski}}}
\bibcite{1807.03653}{{28}{2018}{{Nazabal\ \emph  {et~al.}}}{{Nazabal, Olmos, Ghahramani,\ and\ Valera}}}
\bibcite{aligo_noisecurves}{{29}{}{{ali}}{{}}}
\bibcite{1809.10113}{{30}{2018}{{Khan\ \emph  {et~al.}}}{{Khan, Chatziioannou, Hannam,\ and\ Ohme}}}
\bibcite{2016PhRvD..94d4031S}{{31}{2016}{{{Smith}\ \emph  {et~al.}}}{{{Smith}, {Field}, {Blackburn}, {Haster}, {P{\"u}rrer}, {Raymond},\ and\ {Schmidt}}}}
\bibcite{2019PhRvD..99h4026W}{{32}{2019}{{{Wysocki}\ \emph  {et~al.}}}{{{Wysocki}, {O'Shaughnessy}, {Lange},\ and\ {Fang}}}}
\bibcite{2019PhRvD.100d3030T}{{33}{2019}{{{Talbot}\ \emph  {et~al.}}}{{{Talbot}, {Smith}, {Thrane},\ and\ {Poole}}}}
\bibcite{2020ApJ...892L...3A}{{34}{2020}{{Abbott\ \emph  {et~al.}}}{{Abbott \emph  {et~al.}}}}
\bibcite{2015PhRvD..91h4034L}{{35}{2015}{{{Littenberg}\ and\ {Cornish}}}{{}}}
\bibcite{gallinari1987memoires}{{36}{1987}{{Gallinari\ \emph  {et~al.}}}{{Gallinari, {LeCun}, Thiria,\ and\ Soulie}}}
\bibcite{Siria2020.06.11.144253}{{37}{2020}{{Siria\ \emph  {et~al.}}}{{Siria, Sanou, Mitton, Mwanga, Niang, Sare, Johnson, Foster, Belem, Wynne, Murray-Smith, Ferguson, Gonz{\'a}lez-Jim{\'e}nez, Babayan, Diabat{\'e}, Okumu,\ and\ Baldini}}}
\bibcite{scikit-learn}{{38}{2011}{{Pedregosa\ \emph  {et~al.}}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot,\ and\ Duchesnay}}}
\bibcite{4839047}{{39}{2009}{{{Wang}\ \emph  {et~al.}}}{{{Wang}, {Kulkarni},\ and\ {Verdu}}}}
\bibstyle{apsrev4-1}
\newlabel{LastBibItem}{{39}{13}{}{section*.27}{}}
\newlabel{LastPage}{{}{13}{}{}{}}
